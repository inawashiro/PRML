\section{Introduction}


\subsection{}
Let 
%
\begin{equation}
E(\mathbf{w}) = \frac{1}{2} \sum_{n = 1}^{N} \left( y(x_n, \mathbf{w}) - t_n \right) ^ 2.
\end{equation}
%
To minimise it, setting the derivative to zero gives
%
\begin{equation}
\mathbf{0} = \sum_{n = 1}^{N} \frac{\partial y(x_n, \mathbf{w})}{\partial \mathbf{w}} \left( y(x_n, \mathbf{w}) - t_n \right).
\end{equation}
%
Substituting 
%
\begin{equation}
y(x_n, \mathbf{w}) = \sum_{j = 0}^{M} w_j x_n^j
\end{equation}
%
gives
%
\begin{equation}
0 = \sum_{n = 1}^{N} x_n^i \left( \sum_{j = 0}^{M} w_j x_n^j - t_n \right).
\end{equation}
%
Therefore,
%
\begin{equation}
\sum_{j = 0}^{M} A_{ij} w_j = T_i
\end{equation}
%
where
%
\begin{equation}
\begin{aligned}
A_{ij} &= \sum_{n = 1}^{N} x_n^{i + j}, \\
T_i &= \sum_{n = 1}^{N} x_n^i t_n.
\end{aligned}
\end{equation}


\subsection{}
Let
%
\begin{equation}
\tilde{E}(\mathbf{w}) = \frac{1}{2} \sum_{n = 1}^{N} \left( y(x_n, \mathbf{w}) - t_n \right) ^ 2 + \frac{\lambda}{2} \lVert \mathbf{w} \rVert ^ 2.
\end{equation}
%
To minimise it, setting the derivative to zero gives
%
\begin{equation}
\mathbf{0} = \sum_{n = 1}^{N} \frac{\partial y(x_n, \mathbf{w})}{\partial \mathbf{w}} \left( y(x_n, \mathbf{w}) - t_n \right) + \lambda \mathbf{w}.
\end{equation}
%
Substituting 
%
\begin{equation}
y(x_n, \mathbf{w}) = \sum_{j = 0}^{M} w_j x_n^j
\end{equation}
%
gives
%
\begin{equation}
0 = \sum_{n = 1}^{N} x_n^i \left( \sum_{j = 0}^{M} w_j x_n^j - t_n \right) + \lambda w_i.
\end{equation}
%
Therefore,
%
\begin{equation}
\sum_{j = 0}^{M} \tilde{A}_{ij} w_j = T_i
\end{equation}
%
where
%
\begin{equation}
\begin{aligned}
\tilde{A}_{ij} &= \sum_{n = 1}^{N} x_n^{i + j} + \lambda \delta_{ij}, \\
T_i &= \sum_{n = 1}^{N} x_n^i t_n.
\end{aligned}
\end{equation}


\subsection{}
Let $a$, $o$ and $l$ be the events where an apple, orange and lime are selected respectively.
The probability that an apple is selected is given by
%
\begin{equation}
p(a) = p(a | r) p(r) + p(a | b) p(b) + p(a | g) p(g).
\end{equation}
%
Substituting $p(a | r) = \frac{3}{10}$, $p(r) = \frac{1}{5}$, $p(a | g) = \frac{1}{2}$, $p(r) = \frac{1}{5}$, $p(a | g) = \frac{3}{10}$ and $p(g) = \frac{3}{5}$ gives
%
\begin{equation}
p(a) = \frac{17}{50}.
\end{equation}
%

If an orange is selected, the probability that it came from the geen box is given by
%
\begin{equation}
p(g | o) = \frac{p(g, o)}{p(o)}.
\end{equation}
%
Here,
%
\begin{equation}
\begin{aligned}
p(g, o) &= p(o | g) p(g), \\
p(o) & = p(o | r) p(r) + p(o | b) p(b) + p(o | g) p(g).
\end{aligned}
\end{equation}
%
Substituting $p(o | r) = \frac{2}{5}$, $p(r) = \frac{1}{5}$, $p(o | b) = \frac{1}{2}$, $p(b) = \frac{1}{5}$, $p(o | g) = \frac{3}{10}$ and $p(g) = \frac{3}{5}$ gives $p(g, o) = \frac{9}{50}$ and $p(o) = \frac{9}{25}$.
%
Therefore,
\begin{equation}
p(g | o) = \frac{1}{2}.
\end{equation}


\subsection{}
Let 
%
\begin{equation}
x = g(y)
\end{equation}
%
and $\hat{x}$ and $\hat{y}$ be the locations of the maximum of $p_x(x)$ and $p_y(y)$ respcetively.
Let us assume that there exists $\epsilon > 0$ such that $g'(y) \neq 0$ for $\left| y - \hat{y} \right| < \epsilon$.
Then, differentiating both sides of the transoformation
%
\begin{equation}
p_y(y) = p_x \left( g(y) \right) \abs{g'(y)}
\end{equation}
%
and substituting $y = \hat{y}$ gives
%
\begin{equation}
0 = g'(\hat{y}) p_x' \left( g \left( \hat{y} \right) \right) + p_x \left( g \left( \hat{y} \right) \right) g'' \left( \hat{y} \right).
\end{equation}
%
Therefore, in general,
%
\begin{equation}
\hat{x} \neq g \left( \hat{y} \right).
\end{equation}
%

Here, let us assume that 
%
\begin{equation}
g(y) = a y + b.
\end{equation}
%
Then, differentiating both sides of the transformation and substituting $y = \hat{y}$ gives
%
\begin{equation}
0 = p_x' \left( g \left( \hat{y} \right) \right).
\end{equation}
%
Therefore, 
%
\begin{equation}
\hat{x} = g \left( \hat{y} \right).
\end{equation}
%


\subsection{}
By the definition, 
%
\begin{equation}
{\rm var} f(x) = {\rm E} \left( f(x) - {\rm E} f(x) \right) ^ 2.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
{\rm E} \left( \left( f(x) \right) ^ 2 - 2 f(x) {\rm E} f(x) + \left( {\rm E} f(x) \right) ^ 2 \right) = {\rm E} \left( f(x) \right) ^ 2 - \left( {\rm E} f(x) \right) ^ 2.
\end{equation}
%
Therefore, 
%
\begin{equation}
{\rm var} f(x) = {\rm E} \left( f(x) \right) ^ 2 - \left( {\rm E} f(x) \right) ^ 2.
\end{equation}
%


\subsection{}
\label{subsec_1_6}
By the definition,
%
\begin{equation}
{\rm cov} (x, y) = {\rm E} \left( \left( x - {\rm E} x \right) \left( y - {\rm E} y\right) \right).
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
{\rm E} x y - {\rm E} \left( x {\rm E} y \right) - {\rm E} \left( y {\rm E} x \right) + {\rm E} \left( {\rm E} x {\rm E} y \right) = {\rm E} x y - {\rm E} x {\rm E} y.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\int x y p(x, y) dx dy - \int x p(x) dx \int y p(y) dy.
\end{equation}
%
If $x$ and $y$ are independent, by the definition,
%
\begin{equation}
f(x, y) = f(x) f(y).
\end{equation}
%
Then,
%
\begin{equation}
\int x y p(x, y) dx dy = \int p(x) dx \int p(y) dy.
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm cov} (x, y) = 0.
\end{equation}
%


\subsection{}
\label{subsec_1.7}
Let
%
\begin{equation}
I = \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2 \sigma ^ 2} x ^ 2 \right) dx.
\end{equation}
%
Then
%
\begin{equation}
I ^ 2 = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2 \sigma ^ 2} \left( x ^ 2 + y ^ 2 \right) \right) dx dy.
\end{equation}
%
By the transformation from Cartesian coordinates $(x, y)$ to polar coordinates $(r, \theta)$, the right hand side can be written as
%
\begin{equation}
\int_{0}^{\infty} \int_{0}^{2 \pi} \exp \left( - \frac{1}{2 \sigma ^ 2} r ^ 2 \right) 
\begin{vmatrix}
\cos \theta & - r \sin \theta \\
\sin \theta &  r \cos \theta \\
\end{vmatrix}
dr d\theta
= 2 \pi \int_{0}^{\infty} \exp \left( - \frac{1}{2 \sigma ^ 2} r ^ 2 \right) r dr.
\end{equation}
%
By the transformation $s = \frac{r}{\sigma}$, the right hand side can be written as
%
\begin{equation}
2 \pi \sigma ^ 2 \int_{0}^{\infty} \exp \left( - \frac{1}{2} s ^ 2 \right) s ds = 2 \pi \sigma ^ 2 \left[ - \exp \left( - \frac{1}{2} s ^ 2 \right) \right]_0^\infty.
\end{equation}
%
Therefore, 
%
\begin{equation}
I = \left( 2 \pi \sigma ^ 2 \right) ^ \frac{1}{2}. 
\end{equation}
%

By the definition,
%
\begin{equation}
\mathcal{N} \left( x | \mu, \sigma ^ 2 \right) = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \exp \left( - \frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right).
\end{equation}
%
Then
%
\begin{equation}
\int_{- \infty}^{\infty} \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right) dx.
\end{equation}
%
By the transformation $t = x - \mu$, the right hand side can be written as 
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2 \sigma ^ 2} t ^ 2 \right) dt = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} I.
\end{equation}
%
Therefore,
%
\begin{equation}
\int_{- \infty}^{\infty} \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx = 1.
\end{equation}
%


\subsection{}
Let $x$ be a variable under the Gaussian distribution with mean $\mu$ and variance $\sigma ^ 2$.
Then
%
\begin{equation}
{\rm E} x = \int_{-\infty}^{\infty} x \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx.
\end{equation}
%
By the definition, the right hand side can be written as
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} x \exp \left( -\frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right) dx.
\end{equation}
%
By the transformation $y = x - \mu$, it can be written as 
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} (y + \mu) \exp \left( -\frac{1}{2 \sigma ^ 2}y ^ 2 \right) dy.
\end{equation}
%
Since 
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} y \exp \left( -\frac{1}{2 \sigma ^ 2}y ^ 2 \right) dy = 0,
\end{equation}
%
and
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} \mu \exp \left( -\frac{1}{2 \sigma ^ 2}y ^ 2 \right) dy = \mu \int_{- \infty}^{\infty} \mathcal{N} \left( y | \mu, \sigma ^ 2 \right) dy,
\end{equation}
%
we have
%
\begin{equation}
{\rm E} x = \mu.
\end{equation}
%

By the definition,  
%
\begin{equation}
\int_{- \infty}^{\infty} \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx = 1
\end{equation}
%
can be written as
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} \exp \left( - \frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right) dx = 1.
\end{equation}
%
Differentiating both sides with respect to $\sigma ^ 2$ gives 
%
\begin{equation}
\begin{aligned}
\left( 2 \pi \right) ^ {-\frac{1}{2}} \left( - \frac{1}{2} \right) \left( \sigma ^ 2 \right) ^ {- \frac{3}{2}} \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right) dx \\
+ \left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} \frac{1}{2} \left( \sigma ^ 2 \right) ^ {- 2} (x - \mu) ^ 2 \exp \left( - \frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right) dx = 0.
\end{aligned}
\end{equation}
%
The left hand side can be written as
%
\begin{equation}
\begin{aligned}
- \frac{1}{2} \left( \sigma ^ 2 \right) ^ {- 1} \int_{- \infty}^{\infty} \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx + \frac{1}{2} \left( \sigma ^ 2 \right) ^ {- 2} \int_{- \infty} ^ {\infty} (x - \mu) ^ 2 \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx \\
= - \frac{1}{2} \left( \sigma ^ 2 \right) ^ {- 1} + \frac{1}{2} \left( \sigma ^ 2 \right) ^ {- 2} {\rm var} x.
\end{aligned}
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm var} x = \sigma ^ 2.
\end{equation}
%


\subsection{}
Let
%
\begin{equation}
\mathcal{N} \left( x | \mu, \sigma ^ 2 \right) = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \exp \left( - \frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right).
\end{equation}
%
Setting its derivative with respect to $x$ to zero gives
%
\begin{equation}
0 = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \left( - \frac{1}{\sigma ^ 2} (x - \mu) \right) \exp \left( - \frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right).
\end{equation}
%
Therefore, the mode is given by $\mu$.

Similarly, let
%
\begin{equation}
\mathcal{N} \left( \mathbf{x} | \bm{\mu}, \bm{\Sigma} \right) = \left( 2 \pi \right) ^ {- \frac{D}{2}} |\bm{\Sigma}| ^ {- \frac{1}{2}} \exp \left( - \frac{1}{2} (\mathbf{x} - \bm{\mu}) ^ \intercal \bm{\Sigma} ^ {- 1} (\mathbf{x} - \bm{\mu}) \right).
\end{equation}
%
Setting its derivative with respect to $\mathbf{x}$ to zero gives
%
\begin{equation}
\mathbf{0} = - \left( 2 \pi \right) ^ {- \frac{D}{2}} |\bm{\Sigma}| ^ {- \frac{1}{2}} \left( \bm{\Sigma} ^ {- 1} + \left( \bm{\Sigma} ^ {- 1} \right) ^ \intercal \right) (\mathbf{x} - \bm{\mu}) \exp \left( - \frac{1}{2} (\mathbf{x} - \bm{\mu}) ^ \intercal \bm{\Sigma} ^ {- 1} (\mathbf{x} - \bm{\mu}) \right).
\end{equation}
%
Therefore, the mode is given by $\bm{\mu}$.


\subsection{}
By the definition,
%
\begin{equation}
{\rm E} (x + y) = \int \int (x + y) p(x, y) dx dy.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
\int x \left( \int p(x, y) dy \right) dx + \int y \left( \int p(x, y) dx \right) dy = \int x p(x) dx + \int y p(y) dy.
\end{equation}
%
By the definition, the right hand side can be written as 
%
\begin{equation}
{\rm E} x + {\rm E} y.
\end{equation}
%
Therefore, 
%
\begin{equation}
{\rm E} (x + y) = {\rm E} x + {\rm E} y.
\end{equation}
%

Similarly, by the definition,
%
\begin{equation}
{\rm var} (x + y) = {\rm E} \left( x + y - {\rm E} (x + y) \right) ^ 2
\end{equation}
%
By the result above and the definition, the right hand side can be written as 
%
\begin{equation}
\begin{aligned}
{\rm E} \left( x - {\rm E} x \right) ^ 2 + 2 {\rm E} \left( \left( x - {\rm E} x \right) \left( y - {\rm E} y \right) \right) + {\rm E} \left( y - {\rm E} y \right) ^ 2 \\
= {\rm var} x + 2 {\rm cov} (x, y) + {\rm var} y.
\end{aligned}
\end{equation}
%
If $x$ and $y$ are independent, then 
%
\begin{equation}
{\rm cov} (x, y) = 0,
\end{equation}

%
by \ref{subsec_1_6}. Therefore,
%
\begin{equation}
{\rm var} (x + y) = {\rm var} x + {\rm var} y.
\end{equation}
%


\subsection{}
Let
%
\begin{equation}
\ln p \left( \mathbf{x} | \mu, \sigma ^ 2 \right) = - \frac{N}{2} \ln \left( 2 \pi \sigma ^ 2 \right) - \frac{1}{2 \sigma ^ 2} \sum_{n = 1}^{N} (x_n - \mu) ^ 2.
\end{equation}
%
To maximise it with respect to $\mu$ and $\sigma ^ 2$, setting the partial derivatives to zero gives
%
\begin{equation}
\begin{aligned}
0 &= \frac{1}{\sigma ^ 2} \sum_{n = 1}^{N} (x_n - \mu), \\
0 &= - \frac{N}{2 \sigma ^ 2} + \frac{1}{2 \left( \sigma ^ 2 \right) ^ 2} \sum_{n = 1}^{N} (x_n - \mu) ^ 2.
\end{aligned}
\end{equation}
%
Therefore,
%
\begin{equation}
\begin{aligned}
\mu_{\rm ML} &= \frac{1}{N} \sum_{n = 1}^{N} x_n, \\
\sigma_{\rm ML} ^ 2 &= \frac{1}{N} \sum_{n = 1}^{N} (x_n - \mu_{\rm ML}) ^ 2.
\end{aligned}
\end{equation}
%


\subsection{}
Let $x_m$ and $x_n$ be independent variables. 
Then
%
\begin{equation}
{\rm E} x_m x_n = {\rm E} x_m {\rm E} x_n.
\end{equation}
%
If they are samples from the Gaussian distribution with mean $\mu$ and variance $\sigma ^ 2$, the right hand side is given by $\mu ^ 2$.
On the other hand, by the definition, 
%
\begin{equation}
{\rm E} x_n ^ 2 =  {\rm var} x_n + \left( {\rm E} x_n \right) ^ 2.
\end{equation}
%
If $x_n$ is a sample from the Gaussian distribution with mean $\mu$ and variance $\sigma ^ 2$, the right hand side is given by $\sigma ^ 2 + \mu ^ 2$.
Therefore,
%
\begin{equation}
{\rm E} x_m x_n = \mu ^ 2 + \delta_{mn} \sigma ^ 2.
\end{equation}
%

Here, since 
%
\begin{equation}
\mu_{\rm ML} = \frac{1}{N} \sum_{n = 1}^{N} x_n,
\end{equation}
%
we have
%
\begin{equation}
{\rm E} \mu_{\rm ML} = \frac{1}{N} \sum_{n = 1}^{N} {\rm E} x_n.
\end{equation}
%
Therefore, 
%
\begin{equation}
{\rm E} \mu_{\rm ML} = \mu.
\end{equation}
%
Similarly, since 
%
\begin{equation}
\sigma_{\rm ML}^2 = \frac{1}{N} \sum_{n = 1}^{N} \left( x_n - \mu_{\rm ML} \right) ^ 2,
\end{equation}
%
we have 
%
\begin{equation}
{\rm E} \sigma_{\rm ML}^2 = \frac{1}{N} \sum_{n = 1}^{N} {\rm E} \left( x_n - \mu_{\rm ML} \right) ^ 2.
\end{equation}
%
The right hand side can be writen as 
%
\begin{equation}
\frac{1}{N} \sum_{n = 1}^{N} {\rm E} \left( x_n ^ 2 - 2 \mu_{\rm ML} x_n + \mu_{\rm ML} ^ 2 \right) = \frac{1}{N} \sum_{n = 1}^{N} {\rm E} x_n ^ 2 - \frac{2}{N} {\rm E} \left( \mu_{\rm ML} \left( \sum_{n = 1}^{N} x_n \right) \right) + {\rm E} \mu_{\rm ML} ^ 2.
\end{equation}
%
The first term of the right hand side can be written as 
%
\begin{equation}
\frac{1}{N} \sum_{n = 1}^{N} \left( \mu ^ 2 + \sigma ^ 2 \right) = \mu ^ 2 + \sigma ^ 2,
\end{equation}
%
while the second and third terms can be writen as
%
\begin{equation}
- 2 {\rm E} \mu_{\rm ML} ^ 2 + {\rm E} \mu_{\rm ML} ^ 2 = - {\rm E} \mu_{\rm ML} ^ 2.
\end{equation}
%
Here, 
%
\begin{equation}
{\rm E} \mu_{\rm ML} ^ 2 = {\rm E} \left( \frac{1}{N} \sum_{n = 1}^{N} x_n \right) ^ 2.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
\frac{1}{N ^ 2} \sum_{n = 1}^{N} {\rm E} x_n ^ 2 + \frac{2}{N ^ 2} \sum_{1 \leq m < n \leq N} {\rm E} x_m x_n = \frac{1}{N} (\mu ^ 2 + \sigma ^ 2) + \frac{N - 1}{N} \mu ^ 2.
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm E} \mu_{\rm ML} ^ 2 = \mu ^ 2 + \frac{1}{N} \sigma ^ 2.
\end{equation}
%
Thus,
%
\begin{equation}
{\rm E} \sigma_{\rm ML}^2 = \frac{N - 1}{N} \sigma ^ 2.
\end{equation}
%


\subsection{}
Let $\{ x_n \}$ be a set of variables whose mean is $\mu$ and variance is $\sigma ^ 2$.
Then
%
\begin{equation}
{\rm E} \left( \frac{1}{N} \sum_{n = 1}^{N} \left( x_n - \mu \right) ^ 2 \right) = \frac{1}{N} \sum_{n = 1}^{N} {\rm E} \left( x_n - \mu \right) ^ 2.
\end{equation}
%
The right hand side can be writen as 
%
\begin{equation}
\frac{1}{N} \sum_{n = 1}^{N} {\rm E} \left( x_n ^ 2 - 2 \mu x_n + \mu ^ 2 \right) = \frac{1}{N} \sum_{n = 1}^{N} {\rm E} x_n ^ 2 - \frac{2 \mu}{N}  \sum_{n = 1}^{N} {\rm E} x_n + \mu ^ 2.
\end{equation}
%
The first term of the right hand side can be written as 
%
\begin{equation}
\frac{1}{N} \sum_{n = 1}^{N} \left( \mu ^ 2 + \sigma ^ 2 \right) = \mu ^ 2 + \sigma ^ 2,
\end{equation}
%
while the second term can be writen as
%
\begin{equation}
- \frac{2 \mu}{N} \sum_{n = 1}^{N} \mu = - 2 \mu ^ 2.
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm E} \left( \frac{1}{N} \sum_{n = 1}^{N} \left( x_n - \mu \right) ^ 2 \right) = \sigma ^2.
\end{equation}
%


\subsection{}
Let
%
\begin{equation}
\begin{aligned}
w_{ij}^{\rm S} &= \frac{1}{2} (w_{ij} + w_{ji}), \\
w_{ij}^{\rm A} &= \frac{1}{2} (w_{ij} - w_{ji}).
\end{aligned}
\end{equation}
%
Then
%
\begin{equation}
\begin{aligned}
w_{ij} &= w_{ij}^{\rm S} + w_{ij}^{\rm A}, \\
w_{ij}^{\rm S} &= w_{ji}^{\rm S}, \\
w_{ij}^{\rm A} &= - w_{ji}^{\rm A}.
\end{aligned}
\end{equation}
%
Here,
\begin{equation}
\sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij}^{\rm A} x_i x_j = \frac{1}{2} \sum_{i = 1}^{D} \sum_{j = 1}^{D} (w_{ij} - w_{ji}) x_i x_j.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\frac{1}{2} \left( \sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij} x_i x_j - \sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ji} x_i x_j \right) = 0.
\end{equation}
%
Therefore, 
%
\begin{equation}
\sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij}^{\rm A} x_i x_j = 0.
\end{equation}
%

Additionally,
%
\begin{equation}
\sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij} x_i x_j = \sum_{i = 1}^{D} \sum_{j = 1}^{D} \left( w_{ij}^{\rm S} + w_{ij}^{\rm A} \right) x_i x_j.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij}^{\rm S} x_i x_j + \sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij}^{\rm A} x_i x_j = \sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij}^{\rm S} x_i x_j,
\end{equation}
%
where the result above is used. 
Therefore,
%
\begin{equation}
\sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij} x_i x_j = \sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij}^{\rm S} x_i x_j.
\end{equation}
%

Finally, since the matrix $w_{ij}^{\rm S}$ is $D \times D$ symmetric matrix, its number of independent parameters is $\frac{D (D + 1)}{2}$.


\stepcounter{subsection}


\stepcounter{subsection}


\subsection{}
Let
%
\begin{equation}
\Gamma(x) = \int_{0}^{\infty} u ^ {x - 1} \exp(- u) du.
\end{equation}
%
Then
%
\begin{equation}
\Gamma(x + 1) = \int_{0}^{\infty} u ^ {x} \exp(- u) du.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
\left[ - u ^ x \exp(- u) \right]_{u = 0}^{u = \infty} + \int_{0}^{\infty} x u ^ {x - 1} \exp(- u) du = x \Gamma(x).
\end{equation}
%
Therefore,
%
\begin{equation}
\Gamma( x + 1) = x \Gamma(x).
\end{equation}
%

Since
%
\begin{equation}
\Gamma(1) = \int_{0}^{1} \exp(- u) du,
\end{equation}
%
and the right hand side can be written as $1$,
%
\begin{equation}
\Gamma(1) = 0!.
\end{equation}
%
For a positive integer $x$, let us assume that
%
\begin{equation}
\Gamma(x) = (x - 1)!.
\end{equation}
%
Then,
%
\begin{equation}
\Gamma(x + 1) = x \Gamma(x),
\end{equation}
%
where the right hand side can be written as
%
\begin{equation}
x (x - 1)! = x!.
\end{equation}
%
Therefore,
%
\begin{equation}
\Gamma(x + 1) = x!.
\end{equation}
%
Thus, the assumption is proved by induction on $x$.


\subsection{}
\label{subsec_1.18}
Let us consider the transformation from Cartesian to polar coordinates
%
\begin{equation}
\prod_{i = 1}^{D} \int_{- \infty}^{\infty} \exp \left( - x_i ^ 2 \right) dx_i = S_D \int_{0}^{\infty} \exp \left( - r ^ 2 \right) r ^ {D - 1} dr,
\end{equation}
%
where $S_D$ is the surface area of a sphere of unit raidus in $D$ dimensions. 
By \ref{subsec_1.7}, the left hand side can be written as $\pi ^ \frac{D}{2}$. By the transformation $s = r ^ 2$, the right hand side can be written as
%
\begin{equation}
\frac{S_D}{2} \int_0^\infty \exp(- s) s ^ \frac{D - 1}{2} s ^ {- \frac{1}{2}} ds = \frac{S_D}{2} \Gamma \left( \frac{D}{2} \right).
\end{equation}
%
Therefore,
%
\begin{equation}
S_D = \frac{2 \pi ^ \frac{D}{2}}{\Gamma \left( \frac{D}{2} \right)}.
\end{equation}
%

Additionally, the volume of the sphere can can be written as
%
\begin{equation}
V_D = S_D \int_{0}^{1} r^{D - 1} dr.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
S_D \left[ \frac{r ^ D}{D} \right]_{r = 0}^{r = 1} = \frac{S_D}{D}.
\end{equation}
%
Therefore, 
%
\begin{equation}
V_D = \frac{S_D}{D}.
\end{equation}
%

Finally, the results above reduce to
%
\begin{equation}
\begin{aligned}
S_2 &= \frac{2 \pi}{\Gamma(1)}, \\
V_2 &= \frac{S_2}{2}. 
\end{aligned}
\end{equation}
%
Therefore, 
%
\begin{equation}
\begin{aligned}
S_2 &= 2 \pi, \\
V_2 &= \pi. 
\end{aligned}
\end{equation}
%
Similarly,
%
\begin{equation}
\begin{aligned}
S_3 &= \frac{2 \pi ^ \frac{3}{2}}{\Gamma \left( \frac{3}{2} \right)}, \\
V_3 &= \frac{S_3}{3}. 
\end{aligned}
\end{equation}
%
Therefore, 
%
\begin{equation}
\begin{aligned}
S_3 &= 4 \pi, \\
V_3 &= \frac{4}{3} \pi. 
\end{aligned}
\end{equation}
%


\subsection{}
The volume of a cube of side 2 in $D$ dimensions is $2 ^ D$. 
Therefore, the ratio of the volume of the cocentric sphere of radius 1 divided by the volume of the cube is given by
%
\begin{equation}
\frac{V_D}{2 ^ D} = \frac{\pi ^ \frac{D}{2}}{D 2 ^ {D - 1} \Gamma \left( \frac{D}{2} \right)},
\end{equation}
%
by \ref{subsec_1.18}.

Additionally, by Stering's formula
%
\begin{equation}
\Gamma(x + 1) \simeq (2 \pi) ^ \frac{1}{2} \exp(- x) x ^ {\frac{x + 1}{2}},
\end{equation}
%
the ratio can be approximated as
%
\begin{equation}
\frac{V_D}{2 ^ D} \simeq \frac{\pi ^ \frac{D}{2}}{D 2 ^ {D - 1} (2 \pi) ^ \frac{1}{2} \exp \left( 1 - \frac{D}{2} \right) \left( \frac{D}{2} - 1 \right) ^ \frac{D}{4}}.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
\frac{1}{2 e (2 \pi) ^ \frac{1}{2}} \frac{1}{D} \left( \frac{e ^ 2 \pi ^ 2}{ 8 D - 16} \right) ^ \frac{D}{4}.
\end{equation}
%
Therefore, the ratio goes to zero as $D \to \infty$.

Finally, the ratio of the distance from the center of the cube to one of the corners divided by the perpendicular distance to one of the sides is given by
%
\begin{equation}
\frac{\sqrt{\sum_{i = 1}^{D} 1 ^ 2}}{1} = \sqrt{D}.
\end{equation}
%
Therefore, the ration goes to $\infty$ as $D \to \infty$.


\subsection{}
For a vector $\mathbf{x}$ in $D$ dimensions, let 
%
\begin{equation}
p(\mathbf{x}) = (2 \pi \sigma ^ 2) ^ {- \frac{D}{2}} \exp \left( - \frac{\norm{\mathbf{x}} ^ 2}{2 \sigma ^2} \right).
\end{equation}
%
Integrating both sides from $\norm{\mathbf{x}} = r$ to $\norm{\mathbf{x}} = r + \epsilon$ gives
%
\begin{equation}
\int_{r \leq \norm{\mathbf{x}} \leq r + \epsilon} p(\mathbf{x}) d\mathbf{x} = \int_{r}^{r + \epsilon} \int (2 \pi \sigma ^ 2) ^ {- \frac{D}{2}} \exp \left( - \frac{{r'} ^ 2}{2 \sigma ^2} \right) J dr' d\bm{\phi},
\end{equation}
%
where $\bm{\phi}$ is the vector of the angular components of the polar corrdinate and $J$ is the Jacobian of the transformation from the Cartesian to polar coordinate.
For a sufficiently small $\epsilon$, the right hand side can be approximated as
%
\begin{equation}
\begin{aligned}
(2 \pi \sigma ^ 2) ^ {- \frac{D}{2}} \exp \left( - \frac{r ^ 2}{2 \sigma ^2} \right) \int_{r}^{r + \epsilon} \int J dr' d\bm{\phi} \\
= (2 \pi \sigma ^ 2) ^ {- \frac{D}{2}} \exp \left( - \frac{r ^ 2}{2 \sigma ^2} \right) \int_{r \leq \norm{\mathbf{x}} \leq r + \epsilon} d\mathbf{x}.
\end{aligned}
\end{equation}
%
Therefore,
%
\begin{equation}
\int_{r \leq \norm{\mathbf{x}} \leq r + \epsilon} p(\mathbf{x}) d\mathbf{x} \simeq p(r) \epsilon,
\end{equation}
%
where
%
\begin{equation}
p(r) = (2 \pi \sigma ^ 2) ^ {- \frac{D}{2}} S_D r ^ {D - 1} \exp \left( - \frac{r ^ 2}{2 \sigma ^2} \right),
\end{equation}
%
and $S_D$ is the surface area of a unit sphere in $D$ dimensions.

Secondly, to maximise $p(r)$, setting the derivative to zero gives
%
\begin{equation}
0 = (2 \pi \sigma ^ 2) ^ {- \frac{D}{2}} S_D \left( (D - 1) r ^ {D - 2} - \frac{r ^ D}{\sigma ^ 2} \right) \exp \left( - \frac{r ^ 2}{2 \sigma ^2} \right).
\end{equation}
%
Therefore, $p(r)$ is maximised at a sigle stationary point
%
\begin{equation}
\hat{r} = \sqrt{D - 1} \sigma.
\end{equation}
%

Thirdly, by the expression of $p(r)$ above,
%
\begin{equation}
\frac{p \left( \hat{r} + \epsilon \right)}{p \left( \hat{r} \right)} = \left( \frac{\hat{r} + \epsilon}{\hat{r}} \right) ^ {D - 1} \exp \left( - \frac{2 \hat{r} \epsilon + \epsilon ^ 2}{2 \sigma ^ 2} \right).
\end{equation}
%
Using the expression of $\hat{r}$ above, the right hand side can be written as
%
\begin{equation}
\begin{aligned}
\exp \left( (D - 1) \ln \left( 1 + \frac{\epsilon}{\hat{r}} \right) - \frac{2 \hat{r} \epsilon + \epsilon ^ 2}{2 \sigma ^ 2} \right) \\
= \exp \left( \frac{\hat{r} ^ 2}{\sigma ^ 2} \ln \left( 1 + \frac{\epsilon}{\hat{r}} \right) - \frac{2 \hat{r} \epsilon + \epsilon ^ 2}{2 \sigma ^ 2} \right).
\end{aligned}
\end{equation}
%
By the Taylor series
%
\begin{equation}
\ln (1 + x) = x - \frac{1}{2} x ^ 2 + o \left( x ^ 3 \right),
\end{equation}
%
the right hand side can be approximated as
%
\begin{equation}
\exp \left( \frac{\hat{r} ^ 2}{\sigma ^ 2} \left( \frac{\epsilon}{\hat{r}} - \frac{\epsilon ^ 2}{2 {\hat{r}} ^ 2} \right) - \frac{2 \hat{r} \epsilon + \epsilon ^ 2}{2 \sigma ^ 2} \right) = \exp \left( - \frac{\epsilon ^ 2}{\sigma ^ 2} \right).
\end{equation}
%
Therefore,
%
\begin{equation}
p \left( \hat{r} + \epsilon \right) \simeq p \left( \hat{r} \right) \exp \left( - \frac{\epsilon ^ 2}{\sigma ^ 2} \right).
\end{equation}
%

Finally, let a vector of length $\hat{r}$ be $\hat{\mathbf{r}}$.
Then, by the definition of $p(\mathbf{x})$,
%
\begin{equation}
\frac{p(\mathbf{0})}{p \left( \hat{\mathbf{r}} \right)} = \exp \left( \frac{\hat{r} ^ 2}{2 \sigma ^2} \right).
\end{equation}
%
Substituting the expression of $\hat{r}$ above, the right hand side can be written as $\exp \left( \frac{D - 1}{2} \right)$.
Therefore,
%
\begin{equation}
\frac{p(\mathbf{0})}{p \left( \hat{\mathbf{r}} \right)} = \exp \left( \frac{D - 1}{2} \right).
\end{equation}
%


\subsection{}
If $0 \leq a \leq b$, then
%
\begin{equation}
0 \leq a (b - a).
\end{equation}
%
Therefore,
%
\begin{equation}
a \leq (ab) ^ \frac{1}{2}.
\end{equation}
%

For a two-class classification problem of $\mathbf{x}$, let the classes be $\mathcal{C}_1$ and $\mathcal{C}_2$ and let the decision regions be $\mathcal{R}_1$ and $\mathcal{R}_2$.
Let us choose the decision regions to minimise the probability of misclassification.
Then,
%
\begin{equation}
p(\mathbf{x}, \mathcal{C}_1) > p(\mathbf{x}, \mathcal{C}_2) \Rightarrow \mathbf{x} \in \mathcal{C}_1, 
\end{equation}
%
and
%
\begin{equation}
p(\mathbf{x}, \mathcal{C}_2) > p(\mathbf{x}, \mathcal{C}_1) \Rightarrow \mathbf{x} \in \mathcal{C}_2.
\end{equation}
%
Then, using the inequality above,
%
\begin{equation}
\int_{\mathcal{R}_1} p(\mathbf{x}, \mathcal{C}_2) d\mathbf{x} \leq \int_{\mathcal{R}_1} \left( p(\mathbf{x}, \mathcal{C}_1) p(\mathbf{x}, \mathcal{C}_2) \right) ^ \frac{1}{2} d\mathbf{x},
\end{equation}
%
and
%
\begin{equation}
\int_{\mathcal{R}_2} p(\mathbf{x}, \mathcal{C}_1) d\mathbf{x} \leq \int_{\mathcal{R}_2} \left( p(\mathbf{x}, \mathcal{C}_1) p(\mathbf{x}, \mathcal{C}_2) \right) ^ \frac{1}{2} d\mathbf{x}.
\end{equation}
%
Therefore,
%
\begin{equation}
\int_{\mathcal{R}_1} p(\mathbf{x}, \mathcal{C}_2) d\mathbf{x} + \int_{\mathcal{R}_2} p(\mathbf{x}, \mathcal{C}_1) d\mathbf{x} \leq \int \left( p(\mathbf{x}, \mathcal{C}_1) p(\mathbf{x}, \mathcal{C}_2) \right) ^ \frac{1}{2} d\mathbf{x}.
\end{equation}
%


\subsection{}
Let
%
\begin{equation}
{\rm E} L = \sum_{k} \sum_{j} \int_{\mathcal{R}_j} L_{kj} p(\mathbf{x}, \mathcal{C}_k) d\mathbf{x}.
\end{equation}
%
If
%
\begin{equation}
L_{kj} = 1 - \delta_{kj},
\end{equation}
%
then the right hand side can be written as
%
\begin{equation}
\sum_{k} \sum_{j} \int_{\mathcal{R}_j} \left( p(\mathbf{x}, \mathcal{C}_k) - p(\mathbf{x}, \mathcal{C}_j) \right)  d\mathbf{x} = \sum_{j} \int_{\mathcal{R}_j} \left( \sum_{k} p(\mathbf{x}, \mathcal{C}_k) - p(\mathbf{x}, \mathcal{C}_j) \right)  d\mathbf{x}.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\sum_{j} \int_{\mathcal{R}_j} \left( p(\mathbf{x}) - p(\mathbf{x}, \mathcal{C}_j) \right)  d\mathbf{x} = 1 - \sum_{j} \int_{\mathcal{R}_j} p(\mathbf{x}, \mathcal{C}_j) d\mathbf{x}.
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm E} L = 1 - \sum_{j} \int_{\mathcal{R}_j} p(\mathcal{C}_j | \mathbf{x}) p(\mathbf{x}) d\mathbf{x}.
\end{equation}
%
Thus, minimising ${\rm E} L$ reduces to choosing the criterion to maximise the posterior probatility $p(\mathcal{C}_j | \mathbf{x})$.


\subsection{}
Let
%
\begin{equation}
{\rm E} L = \sum_{k} \sum_{j} \int_{\mathcal{R}_j} L_{kj} p(\mathbf{x}, \mathcal{C}_k) d\mathbf{x}.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\sum_{j} \int_{\mathcal{R}_j} \sum_{k} L_{kj} p(\mathbf{x}, \mathcal{C}_k) d\mathbf{x} = \sum_{j} \int_{\mathcal{R}_j} \left( \sum_{k} L_{kj} p(\mathcal{C}_k | \mathbf{x}) \right) p(\mathbf{x}) d\mathbf{x}.
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm E} L = \sum_{j} \int_{\mathcal{R}_j} \left( \sum_{k} L_{kj} p(\mathcal{C}_k | \mathbf{x}) \right) p(\mathbf{x}) d\mathbf{x}.
\end{equation}
%
Thus, mimising ${\rm E} L$ reduces to choosing to minimise $\sum_{k} L_{kj} p(\mathcal{C}_k | \mathbf{x})$.


\stepcounter{subsection}


\subsection{}
Let
%
\begin{equation}
{\rm E} L\left( \mathbf{t}, \mathbf{y} (\mathbf{x}) \right) = \int \int \lVert \mathbf{y}(\mathbf{x}) - \mathbf{t} \rVert ^ 2 p(\mathbf{x}, \mathbf{t}) d\mathbf{x} d\mathbf{t}.
\end{equation}
%
Then
%
\begin{equation}
\frac{\delta {\rm E} L\left( \mathbf{t}, \mathbf{y} (\mathbf{x}) \right)}{\delta \mathbf{y} (\mathbf{x})} = 2 \int \left( \mathbf{y} (\mathbf{x}) - \mathbf{t} \right) p(\mathbf{x}, \mathbf{t}) d\mathbf{t}.
\end{equation}
%
To minimise ${\rm E} L\left( \mathbf{t}, \mathbf{y} (\mathbf{x}) \right)$, setting the left hand side to zero gives
%
\begin{equation}
\mathbf{0} = \int \left( \mathbf{y}(\mathbf{x}) - \mathbf{t} \right) p(\mathbf{t} | \mathbf{x}) d\mathbf{t}.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\mathbf{y}(\mathbf{x}) \int p(\mathbf{t} | \mathbf{x}) d\mathbf{t} - \int \mathbf{t} p(\mathbf{t} | \mathbf{x}) d\mathbf{t} = \mathbf{y}(\mathbf{x}) - {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}).
\end{equation}
%
Thus,
%
\begin{equation}
\mathbf{y}(\mathbf{x}) = {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}).
\end{equation}
%

Finally, for a single target variable $t$, it reduces to 
%
\begin{equation}
\mathbf{y}(\mathbf{x}) = {\rm E}_t (t | \mathbf{x}).
\end{equation}
%


\subsection{}
Let
%
\begin{equation}
{\rm E} L\left( \mathbf{t}, \mathbf{y} (\mathbf{x}) \right) = \int \int \lVert \mathbf{y}(\mathbf{x}) - \mathbf{t} \rVert ^ 2 p(\mathbf{x}, \mathbf{t}) d\mathbf{x} d\mathbf{t}.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\begin{aligned}
\int \int \lVert \mathbf{y}(\mathbf{x}) - {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) + {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) - \mathbf{t} \rVert ^ 2 p(\mathbf{x}, \mathbf{t}) d\mathbf{x} d\mathbf{t} \\ 
= \int \int \lVert \mathbf{y}(\mathbf{x}) - {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) \rVert ^ 2 p(\mathbf{x}, \mathbf{t}) d\mathbf{x} d\mathbf{t} \\
+ 2 \int \int \left( \mathbf{y}(\mathbf{x}) - {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) \right) ^ \intercal \left( {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) - \mathbf{t} \right) p(\mathbf{x}, \mathbf{t}) d\mathbf{x} d\mathbf{t} \\
+ \int \int \lVert {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) - \mathbf{t} \rVert ^ 2 p(\mathbf{x}, \mathbf{t}) d\mathbf{x} d\mathbf{t}.
\end{aligned}
\end{equation}
%
Let us look at each term of the right hand side.
The first term can be written as
%
\begin{equation}
\int \lVert \mathbf{y}(\mathbf{x}) - {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) \rVert ^ 2 \left( \int p(\mathbf{x}, \mathbf{t}) d\mathbf{t} \right) d\mathbf{x} = \int \lVert \mathbf{y}(\mathbf{x}) - {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) \rVert ^ 2 p(\mathbf{x}) d\mathbf{x}.
\end{equation}
%
Additionally, the second term can be written as
%
\begin{equation}
2 \int \left( \mathbf{y}(\mathbf{x}) - {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) \right) ^ \intercal \left( \int \left( {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) - \mathbf{t} \right) p(\mathbf{t} | \mathbf{x}) d\mathbf{t} \right) p(\mathbf{x}) d\mathbf{x}.
\end{equation}
%
Since
%
\begin{equation}
\begin{aligned}
\int {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) p(\mathbf{t} | \mathbf{x}) d\mathbf{t} &= {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) \frac{\int p(\mathbf{x}, \mathbf{t}) d\mathbf{t}}{p(\mathbf{x})}, \\
\int \mathbf{t} p(\mathbf{t} | \mathbf{x}) d\mathbf{t} &= {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}),
\end{aligned}
\end{equation}
%
the second term is zero.
Finally, the third term can be written as
%
\begin{equation}
\int \left( \int \lVert {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) - \mathbf{t} \rVert ^ 2 p(\mathbf{t} | \mathbf{x}) d\mathbf{t} \right) p(\mathbf{x}) d\mathbf{x} = \int {\rm var}_{\mathbf{t}} (\mathbf{t} | \mathbf{x}) p(\mathbf{x}) d\mathbf{x}.
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm E} L\left( \mathbf{t}, \mathbf{y} (\mathbf{x}) \right) = \int \lVert \mathbf{y}(\mathbf{x}) - {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) \rVert ^ 2 p(\mathbf{x}) d\mathbf{x} + \int {\rm var}_{\mathbf{t}} (\mathbf{t} | \mathbf{x}) p(\mathbf{x}) d\mathbf{x}.
\end{equation}
%
Thus, ${\rm E} L\left( \mathbf{t}, \mathbf{y} (\mathbf{x}) \right)$ is mimimised if
%
\begin{equation}
\mathbf{y} (\mathbf{x}) = {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}).
\end{equation}
%.


\subsection{}
Let
%
\begin{equation}
{\rm E} L_q = \int \int \abs{y(\mathbf{x}) - t} ^ q p(\mathbf{x}, t) d\mathbf{x} dt.
\end{equation}
%
Then
%
\begin{equation}
\frac{\delta {\rm E} L_q}{\delta y (\mathbf{x})} = \int q \abs{y(\mathbf{x}) - t} ^ {q - 1} {\rm sign} (y(\mathbf{x}) - t) p(\mathbf{x}, t) dt.
\end{equation}
%
To minimise ${\rm E} L_q$, setting the left hand side to zero gives
%
\begin{equation}
0 = \int \abs{y(\mathbf{x}) - t} ^ {q - 1} {\rm sign} (y(\mathbf{x}) - t) p(t | \mathbf{x}) dt.
\end{equation}
%
This is the condition that $y(\mathbf{x})$ must satisfy in order to minimise ${\rm E} L_q$.

If $q = 1$, the condition can be written as
%
\begin{equation}
0 = \int_{y(\mathbf{x})}^{\infty} p(t | \mathbf{x}) dt - \int_{- \infty}^{y(\mathbf{x})} p(t | \mathbf{x}) dt.
\end{equation}
%
Therefore, $y(\mathbf{x})$ is given by the conditional median.


\subsection{}
Let us assume that
%
\begin{equation}
p(x, y) = p(x) p(y) \Rightarrow h(x, y) = h(x) + h(y).
\end{equation}
%
Let $h(p)$ be a function to relate $h$ and $p$.
Then
%
\begin{equation}
h \left( p ^ 2 \right) = h(p) + h(p).
\end{equation}
%
Therefore,
%
\begin{equation}
h \left( p ^ 2 \right) = 2 h(p).
\end{equation}
%
Let us assume that, for a positive integer $n$,
%
\begin{equation}
h \left( p ^ n \right) = n h(p).
\end{equation}
%
Then, by the first assumption,
%
\begin{equation}
h \left( p ^ {n + 1} \right) = h \left( p ^ n \right) + h(p).
\end{equation}
%
Therefore,
%
\begin{equation}
h \left( p ^ {n + 1} \right) = (n + 1) h(p).
\end{equation}
%
Thus, the second assumption is proved by induction on $n$.

Additionally, for positive integers $m$ and $n$, 
%
\begin{equation}
h \left( p ^ n \right) = h \left( p ^ {\frac{n}{m} m} \right).
\end{equation}
%
By the second assumption, the left hand side can be written as $n h(p)$.
By the first assumption, the right hand side can be written as $m h \left( p ^ \frac{n}{m} \right)$.
Therefore,
%
\begin{equation}
h \left( p ^ \frac{n}{m} \right) = \frac{n}{m} h(p).
\end{equation}
%

Finally, by the continuity, for a positive real number $a$,
%
\begin{equation}
h \left( p ^a \right) = a h(p).
\end{equation}
%
Differentiating both sides with respect to $a$ and substituting $a = 1$ gives
%
\begin{equation}
(p \ln p) h' (p) = h(p).
\end{equation}
%
Therefore,
%
\begin{equation}
\int \frac{h'(p)}{h(p)} dp = \int \frac{1}{p \ln p} dp + C,
\end{equation}
%
where $C$ is a constant.
Ignorting the constants, the left hand side can be written as $\ln h(p)$ and the right hand side can be written as $\ln (\ln p)$.
Thus,
%
\begin{equation}
h(p) \propto \ln p.
\end{equation}
%


\subsection{}
Let $x$ be an $M$-state discrete random variable.
Then, by the definition,
%
\begin{equation}
{\rm H} (x) = - \sum_{i = 1}^{M} p(x_i) \ln p(x_i),
\end{equation}
%
where
%
\begin{equation}
\sum_{i = 1}^{M} p(x_i) = 1.
\end{equation}
%
By Jensen's inequality,
%
\begin{equation}
\sum_{i = 1}^{M} p(x_i) \ln \frac{1}{p(x_i)} \leq \ln \left( \sum_{i = 1}^{M} 1 \right).
\end{equation}
Therefore,
%
\begin{equation}
{\rm H} (x) \leq \ln M.
\end{equation}
%


\subsection{}
Let 
%
\begin{equation}
\begin{aligned}
p(x) &= \mathcal{N} \left( x | \mu, \sigma ^2 \right), \\
q(x) &= \mathcal{N} \left( x | m, s ^2 \right).
\end{aligned}
\end{equation}
%
Then, by the definition,
%
\begin{equation}
{\rm KL} \left( p || q \right) = - \int p(x) \ln \frac{q(x)}{p(x)} dx.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
\begin{aligned}
&- \int_{- \infty}^{\infty} p(x) \ln \frac{\left( 2 \pi s ^ 2 \right) ^ {- \frac{1}{2}} \exp \left( - \frac{(x - m) ^ 2}{2 s ^ 2} \right)}{\left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \exp \left( - \frac{(x - \mu) ^ 2}{2 \sigma ^ 2} \right)} dx \\
& = - \int_{- \infty}^{\infty} p(x) \left( -\frac{1}{2} \ln \frac{s ^ 2}{\sigma ^ 2} - \frac{(x - m) ^ 2}{2 s ^ 2} + \frac{(x - \mu) ^ 2}{2 \sigma ^ 2} \right) dx.
\end{aligned}
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\ln \frac{s}{\sigma} \int_{- \infty}^{\infty} p(x) dx + \frac{1}{2 s ^2} \int_{- \infty}^{\infty} (x - m) ^ 2 p(x) dx - \frac{1}{2 \sigma ^2} \int_{- \infty}^{\infty} (x - \mu) ^ 2 p(x) dx.
\end{equation}
%
The first term can be written as $\ln \frac{s}{\sigma}$.
The second term can be written as
%
\begin{equation}
\frac{1}{2 s ^2} \int_{- \infty}^{\infty} (x - \mu + \mu - m) ^ 2 p(x) dx = \frac{\sigma ^ 2 + (\mu - m) ^ 2}{2 s ^ 2}.
\end{equation}
%
The third term can be written as $- \frac{1}{2}$.
Therefore,
%
\begin{equation}
{\rm KL} \left( p || q \right) = \ln \frac{s}{\sigma} + \frac{\sigma ^ 2 + (\mu - m) ^ 2}{2 s ^ 2} - \frac{1}{2}. 
\end{equation}
%































