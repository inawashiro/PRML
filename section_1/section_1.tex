\section{Introduction}


\subsection{}
To minimise 
%
\begin{equation}
E(\mathbf{w}) = \frac{1}{2} \sum_{n = 1}^{N} \left( y(x_n, \mathbf{w}) - t_n \right) ^ 2,
\end{equation}
%
setting its derivative to zero gives
%
\begin{equation}
\mathbf{0} = \sum_{n = 1}^{N} \frac{\partial y(x_n, \mathbf{w})}{\partial \mathbf{w}} \left( y(x_n, \mathbf{w}) - t_n \right).
\end{equation}
%
Substituting 
%
\begin{equation}
y(x_n, \mathbf{w}) = \sum_{j = 0}^{M} w_j x_n^j
\end{equation}
%
gives
%
\begin{equation}
0 = \sum_{n = 1}^{N} x_n^i \left( \sum_{j = 0}^{M} w_j x_n^j - t_n \right).
\end{equation}
%
Therefore,
%
\begin{equation}
\sum_{j = 0}^{M} A_{ij} w_j = T_i
\end{equation}
%
where
%
\begin{equation}
\begin{aligned}
A_{ij} &= \sum_{n = 1}^{N} x_n^{i + j}, \\
T_i &= \sum_{n = 1}^{N} x_n^i t_n.
\end{aligned}
\end{equation}


\subsection{}
To minimise 
%
\begin{equation}
\tilde{E}(\mathbf{w}) = \frac{1}{2} \sum_{n = 1}^{N} \left( y(x_n, \mathbf{w}) - t_n \right) ^ 2 + \frac{\lambda}{2} \lVert \mathbf{w} \rVert ^ 2,
\end{equation}
%
setting its derivative to zero gives
%
\begin{equation}
\mathbf{0} = \sum_{n = 1}^{N} \frac{\partial y(x_n, \mathbf{w})}{\partial \mathbf{w}} \left( y(x_n, \mathbf{w}) - t_n \right) + \lambda \mathbf{w}.
\end{equation}
%
Substituting 
%
\begin{equation}
y(x_n, \mathbf{w}) = \sum_{j = 0}^{M} w_j x_n^j
\end{equation}
%
gives
%
\begin{equation}
0 = \sum_{n = 1}^{N} x_n^i \left( \sum_{j = 0}^{M} w_j x_n^j - t_n \right) + \lambda w_i.
\end{equation}
%
Therefore,
%
\begin{equation}
\sum_{j = 0}^{M} \tilde{A}_{ij} w_j = T_i
\end{equation}
%
where
%
\begin{equation}
\begin{aligned}
\tilde{A}_{ij} &= \sum_{n = 1}^{N} x_n^{i + j} + \lambda \delta_{ij}, \\
T_i &= \sum_{n = 1}^{N} x_n^i t_n.
\end{aligned}
\end{equation}


\subsection{}
Let $a$, $o$ and $l$ be the events where an apple, orange and lime are selected respectively.
The probability that an apple is selected is given by
%
\begin{equation}
p(a) = p(a | r) p(r) + p(a | b) p(b) + p(a | g) p(g).
\end{equation}
%
Substituting $p(a | r) = \frac{3}{10}$, $p(r) = \frac{1}{5}$, $p(a | g) = \frac{1}{2}$, $p(r) = \frac{1}{5}$, $p(a | g) = \frac{3}{10}$ and $p(g) = \frac{3}{5}$ gives
%
\begin{equation}
p(a) = \frac{17}{50}.
\end{equation}
%

If an orange is selected, the probability that it came from the geen box is given by
%
\begin{equation}
p(g | o) = \frac{p(g, o)}{p(o)}.
\end{equation}
%
Here,
%
\begin{equation}
\begin{aligned}
p(g, o) &= p(o | g) p(g), \\
p(o) & = p(o | r) p(r) + p(o | b) p(b) + p(o | g) p(g).
\end{aligned}
\end{equation}
%
Substituting $p(o | r) = \frac{2}{5}$, $p(r) = \frac{1}{5}$, $p(o | b) = \frac{1}{2}$, $p(b) = \frac{1}{5}$, $p(o | g) = \frac{3}{10}$ and $p(g) = \frac{3}{5}$ gives $p(g, o) = \frac{9}{50}$ and $p(o) = \frac{9}{25}$.
%
Therefore,
\begin{equation}
p(g | o) = \frac{1}{2}.
\end{equation}


\stepcounter{subsection}


\subsection{}
By the definition, 
%
\begin{equation}
{\rm var} f(x) = {\rm E} \left( f(x) - {\rm E} f(x) \right) ^ 2.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
{\rm E} \left( \left( f(x) \right) ^ 2 - 2 f(x) {\rm E} f(x) + \left( {\rm E} f(x) \right) ^ 2 \right) = {\rm E} \left( f(x) \right) ^ 2 - \left( {\rm E} f(x) \right) ^ 2.
\end{equation}
%
Therefore, 
%
\begin{equation}
{\rm var} f(x) = {\rm E} \left( f(x) \right) ^ 2 - \left( {\rm E} f(x) \right) ^ 2.
\end{equation}
%


\subsection{}
\label{subsec_1_6}
By the definition,
%
\begin{equation}
{\rm cov} (x, y) = {\rm E} \left( \left( x - {\rm E} x \right) \left( y - {\rm E} y\right) \right).
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
{\rm E} x y - {\rm E} \left( x {\rm E} y \right) - {\rm E} \left( y {\rm E} x \right) + {\rm E} \left( {\rm E} x {\rm E} y \right) = {\rm E} x y - {\rm E} x {\rm E} y.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\int x y p(x, y) dx dy - \int x p(x) dx \int y p(y) dy.
\end{equation}
%
If $x$ and $y$ are independent, by the definition,
%
\begin{equation}
f(x, y) = f(x) f(y).
\end{equation}
%
Then,
%
\begin{equation}
\int x y p(x, y) dx dy = \int p(x) dx \int p(y) dy.
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm cov} (x, y) = 0.
\end{equation}
%


\subsection{}
Let
%
\begin{equation}
I = \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2 \sigma ^ 2} x ^ 2 \right) dx.
\end{equation}
%
Then
%
\begin{equation}
I ^ 2 = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2 \sigma ^ 2} \left( x ^ 2 + y ^ 2 \right) \right) dx dy.
\end{equation}
%
By the transformation from Cartesian coordinates $(x, y)$ to polar coordinates $(r, \theta)$, the right hand side can be written as
%
\begin{equation}
\int_{0}^{\infty} \int_{0}^{2 \pi} \exp \left( - \frac{1}{2 \sigma ^ 2} r ^ 2 \right) 
\begin{vmatrix}
\cos \theta & - r \sin \theta \\
\sin \theta &  r \cos \theta \\
\end{vmatrix}
dr d\theta
= 2 \pi \int_{0}^{\infty} \exp \left( - \frac{1}{2 \sigma ^ 2} r ^ 2 \right) r dr.
\end{equation}
%
By the transformation $s = \frac{r}{\sigma}$, the right hand side can be written as
%
\begin{equation}
2 \pi \sigma ^ 2 \int_{0}^{\infty} \exp \left( - \frac{1}{2} s ^ 2 \right) s ds = 2 \pi \sigma ^ 2 \left[ - \exp \left( - \frac{1}{2} s ^ 2 \right) \right]_0^\infty.
\end{equation}
%
Therefore, 
%
\begin{equation}
I = \left( 2 \pi \sigma ^ 2 \right) ^ \frac{1}{2}. 
\end{equation}
%

By the definition,
%
\begin{equation}
\mathcal{N} \left( x | \mu, \sigma ^ 2 \right) = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \exp \left( - \frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right).
\end{equation}
%
Then
%
\begin{equation}
\int_{- \infty}^{\infty} \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right) dx.
\end{equation}
%
By the transformation $t = x - \mu$, the right hand side can be written as 
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2 \sigma ^ 2} t ^ 2 \right) dt = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} I.
\end{equation}
%
Therefore,
%
\begin{equation}
\int_{- \infty}^{\infty} \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx = 1.
\end{equation}
%


\subsection{}
If $x$ is under the Gaussian distribution with mean $\mu$ and variance $\sigma ^ 2$, then
%
\begin{equation}
{\rm E} x = \int_{-\infty}^{\infty} x \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx.
\end{equation}
%
By the definition, the right hand side can be written as
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} x \exp \left( -\frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right) dx.
\end{equation}
%
By the transformation $y = x - \mu$, it can be written as 
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} (y + \mu) \exp \left( -\frac{1}{2 \sigma ^ 2}y ^ 2 \right) dy.
\end{equation}
%
Since 
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} y \exp \left( -\frac{1}{2 \sigma ^ 2}y ^ 2 \right) dy = 0,
\end{equation}
%
and
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} \mu \exp \left( -\frac{1}{2 \sigma ^ 2}y ^ 2 \right) dy = \mu \int_{- \infty}^{\infty} \mathcal{N} \left( y | \mu, \sigma ^ 2 \right) dy,
\end{equation}
%
we have
%
\begin{equation}
{\rm E} x = \mu.
\end{equation}
%

By the definition,  
%
\begin{equation}
\int_{- \infty}^{\infty} \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx = 1
\end{equation}
%
can be written as
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} \exp \left( - \frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right) dx = 1.
\end{equation}
%
Differentiating both sides with respect to $\sigma ^ 2$ gives 
%
\begin{equation}
\begin{aligned}
\left( 2 \pi \right) ^ {-\frac{1}{2}} \left( - \frac{1}{2} \right) \left( \sigma ^ 2 \right) ^ {- \frac{3}{2}} \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right) dx \\
+ \left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} \frac{1}{2} \left( \sigma ^ 2 \right) ^ {- 2} (x - \mu) ^ 2 \exp \left( - \frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right) dx = 0.
\end{aligned}
\end{equation}
%
The left hand side can be written as
%
\begin{equation}
\begin{aligned}
- \frac{1}{2} \left( \sigma ^ 2 \right) ^ {- 1} \int_{- \infty}^{\infty} \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx + \frac{1}{2} \left( \sigma ^ 2 \right) ^ {- 2} \int_{- \infty} ^ {\infty} (x - \mu) ^ 2 \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx \\
= - \frac{1}{2} \left( \sigma ^ 2 \right) ^ {- 1} + \frac{1}{2} \left( \sigma ^ 2 \right) ^ {- 2} {\rm var} x.
\end{aligned}
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm var} x = \sigma ^ 2.
\end{equation}
%


\subsection{}
By the definition,
%
\begin{equation}
\mathcal{N} \left( x | \mu, \sigma ^ 2 \right) = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \exp \left( - \frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right).
\end{equation}
%
Setting its derivative to zero gives
%
\begin{equation}
0 = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \left( - \frac{1}{\sigma ^ 2} (x - \mu) \right) \exp \left( - \frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right).
\end{equation}
%
Therefore, the mode is given by $\mu$.

Similarly, by the definition,
%
\begin{equation}
\mathcal{N} \left( \mathbf{x} | \bm{\mu}, \bm{\Sigma} \right) = \left( 2 \pi \right) ^ {- \frac{D}{2}} |\bm{\Sigma}| ^ {- \frac{1}{2}} \exp \left( - \frac{1}{2} (\mathbf{x} - \bm{\mu}) ^ \intercal \bm{\Sigma} ^ {- 1} (\mathbf{x} - \bm{\mu}) \right).
\end{equation}
%
Setting its derivative to zero gives
%
\begin{equation}
\mathbf{0} = \left( 2 \pi \right) ^ {- \frac{D}{2}} |\bm{\Sigma}| ^ {- \frac{1}{2}} \left( - \bm{\Sigma} ^ {- 1} (\mathbf{x} - \bm{\mu}) \right) \exp \left( - \frac{1}{2} (\mathbf{x} - \bm{\mu}) ^ \intercal \bm{\Sigma} ^ {- 1} (\mathbf{x} - \bm{\mu}) \right).
\end{equation}
%
Therefore, the mode is given by $\bm{\mu}$.


\subsection{}
By the definition,
%
\begin{equation}
{\rm E} (x + y) = \int \int (x + y) p(x, y) dx dy.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
\int x \left( \int p(x, y) dy \right) dx + \int y \left( \int p(x, y) dx \right) dy = \int x p(x) dx + \int y p(y) dy.
\end{equation}
%
By the definition, the right hand side can be written as 
%
\begin{equation}
{\rm E} x + {\rm E} y.
\end{equation}
%
Therefore, 
%
\begin{equation}
{\rm E} (x + y) = {\rm E} x + {\rm E} y.
\end{equation}
%

Similarly, by the definition,
%
\begin{equation}
{\rm var} (x + y) = {\rm E} \left( x + y - {\rm E} (x + y) \right) ^ 2
\end{equation}
%
By the result above and the definition, the right hand side can be written as 
%
\begin{equation}
\begin{aligned}
{\rm E} \left( x - {\rm E} x \right) ^ 2 + 2 {\rm E} \left( \left( x - {\rm E} x \right) \left( y - {\rm E} y \right) \right) + {\rm E} \left( y - {\rm E} y \right) ^ 2 \\
= {\rm var} x + 2 {\rm cov} (x, y) + {\rm var} y.
\end{aligned}
\end{equation}
%
If $x$ and $y$ are independent, then 
%
\begin{equation}
{\rm cov} (x, y) = 0,
\end{equation}

%
by \ref{subsec_1_6}. Therefore,
%
\begin{equation}
{\rm var} (x + y) = {\rm var} x + {\rm var} y.
\end{equation}
%


\subsection{}
To maximise 
%
\begin{equation}
\ln p \left( \mathbf{x} | \mu, \sigma ^ 2 \right) = - \frac{N}{2} \ln \left( 2 \pi \sigma ^ 2 \right) - \frac{1}{2 \sigma ^ 2} \sum_{n = 1}^{N} (x_n - \mu) ^ 2
\end{equation}
%
with respect to $\mu$, setting the partial derivative to zero gives
%
\begin{equation}
0 = \frac{1}{\sigma ^ 2} \sum_{n = 1}^{N} (x_n - \mu).
\end{equation}
%
Therefore,
%
\begin{equation}
\mu_{\rm ML} = \frac{1}{N} \sum_{n = 1}^{N} x_n.
\end{equation}
%

Similarly, to maximise the log likelyhood with respect to $\sigma ^ 2$, setting the partial derivative to zero gives
%
\begin{equation}
0 = - \frac{N}{2 \sigma ^ 2} + \frac{1}{2 \left( \sigma ^ 2 \right) ^ 2} \sum_{n = 1}^{N} (x_n - \mu) ^ 2.
\end{equation}
%
Therefore,
%
\begin{equation}
\sigma_{\rm ML} ^ 2 = \frac{1}{N} \sum_{n = 1}^{N} (x_n - \mu) ^ 2.
\end{equation}
%


\subsection{}
If $x_m$ and $x_n$ are independent, then
%
\begin{equation}
{\rm E} x_m x_n = {\rm E} x_m {\rm E} x_n.
\end{equation}
%
If they are samples from the Gaussian distribution with mean $\mu$ and variance $\sigma ^ 2$, the right hand side is given by $\mu ^ 2$.
On the other hand, by the definition, 
%
\begin{equation}
{\rm E} x_n ^ 2 =  {\rm var} x_n + \left( {\rm E} x_n \right) ^ 2.
\end{equation}
%
If $x_n$ is a sample from the Gaussian distribution with mean $\mu$ and variance $\sigma ^ 2$, the right hand side is given by $\sigma ^ 2 + \mu ^ 2$.
Therefore,
%
\begin{equation}
{\rm E} x_m x_n = \mu ^ 2 + \delta_{mn} \sigma ^ 2.
\end{equation}
%

Here, since 
%
\begin{equation}
\mu_{\rm ML} = \frac{1}{N} \sum_{n = 1}^{N} x_n,
\end{equation}
%
we have
%
\begin{equation}
{\rm E} \mu_{\rm ML} = \frac{1}{N} \sum_{n = 1}^{N} {\rm E} x_n.
\end{equation}
%
Therefore, 
%
\begin{equation}
{\rm E} \mu_{\rm ML} = \mu.
\end{equation}
%
Similarly, since 
%
\begin{equation}
\sigma_{\rm ML}^2 = \frac{1}{N} \sum_{n = 1}^{N} \left( x_n - \mu_{\rm ML} \right) ^ 2,
\end{equation}
%
we have 
%
\begin{equation}
{\rm E} \sigma_{\rm ML}^2 = \frac{1}{N} \sum_{n = 1}^{N} {\rm E} \left( x_n - \mu_{\rm ML} \right) ^ 2.
\end{equation}
%
The right hand side can be writen as 
%
\begin{equation}
\frac{1}{N} \sum_{n = 1}^{N} {\rm E} \left( x_n ^ 2 - 2 \mu_{\rm ML} x_n + \mu_{\rm ML} ^ 2 \right) = \frac{1}{N} \sum_{n = 1}^{N} {\rm E} x_n ^ 2 - \frac{2}{N} {\rm E} \left( \mu_{\rm ML} \left( \sum_{n = 1}^{N} x_n \right) \right) + {\rm E} \mu_{\rm ML} ^ 2.
\end{equation}
%
The first term of the right hand side can be written as 
%
\begin{equation}
\frac{1}{N} \sum_{n = 1}^{N} \left( \mu ^ 2 + \sigma ^ 2 \right) = \mu ^ 2 + \sigma ^ 2,
\end{equation}
%
while the second and third terms can be writen as
%
\begin{equation}
- 2 {\rm E} \mu_{\rm ML} ^ 2 + {\rm E} \mu_{\rm ML} ^ 2 = - {\rm E} \mu_{\rm ML} ^ 2.
\end{equation}
%
Here, 
%
\begin{equation}
{\rm E} \mu_{\rm ML} ^ 2 = {\rm E} \left( \frac{1}{N} \sum_{n = 1}^{N} x_n \right) ^ 2.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
\frac{1}{N ^ 2} \sum_{n = 1}^{N} {\rm E} x_n ^ 2 + \frac{2}{N ^ 2} \sum_{1 \leq m < n \leq N} {\rm E} x_m x_n = \frac{1}{N} (\mu ^ 2 + \sigma ^ 2) + \frac{N - 1}{N} \mu ^ 2.
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm E} \mu_{\rm ML} ^ 2 = \mu ^ 2 + \frac{1}{N} \sigma ^ 2.
\end{equation}
%
Thus,
%
\begin{equation}
{\rm E} \sigma_{\rm ML}^2 = \frac{N - 1}{N} \sigma ^ 2.
\end{equation}
%














