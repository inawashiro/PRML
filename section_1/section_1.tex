\section{Introduction}


\subsection{}
\label{1.1}
Let 
%
\begin{equation}
E(\mathbf{w}) = \frac{1}{2} \sum_{n = 1}^{N} \left( y(x_n, \mathbf{w}) - t_n \right) ^ 2.
\end{equation}
%
To minimise it, setting the derivative to zero gives
%
\begin{equation}
\mathbf{0} = \sum_{n = 1}^{N} \frac{\partial y(x_n, \mathbf{w})}{\partial \mathbf{w}} \left( y(x_n, \mathbf{w}) - t_n \right).
\end{equation}
%
Substituting 
%
\begin{equation}
y(x_n, \mathbf{w}) = \sum_{j = 0}^{M} w_j x_n^j
\end{equation}
%
gives
%
\begin{equation}
0 = \sum_{n = 1}^{N} x_n^i \left( \sum_{j = 0}^{M} w_j x_n^j - t_n \right).
\end{equation}
%
Therefore,
%
\begin{equation}
\sum_{j = 0}^{M} A_{ij} w_j = T_i
\end{equation}
%
where
%
\begin{equation}
\begin{aligned}
A_{ij} &= \sum_{n = 1}^{N} x_n^{i + j}, \\
T_i &= \sum_{n = 1}^{N} x_n^i t_n.
\end{aligned}
\end{equation}


\subsection{}
\label{1.2}
Let
%
\begin{equation}
\tilde{E}(\mathbf{w}) = \frac{1}{2} \sum_{n = 1}^{N} \left( y(x_n, \mathbf{w}) - t_n \right) ^ 2 + \frac{\lambda}{2} \lVert \mathbf{w} \rVert ^ 2.
\end{equation}
%
To minimise it, setting the derivative to zero gives
%
\begin{equation}
\mathbf{0} = \sum_{n = 1}^{N} \frac{\partial y(x_n, \mathbf{w})}{\partial \mathbf{w}} \left( y(x_n, \mathbf{w}) - t_n \right) + \lambda \mathbf{w}.
\end{equation}
%
Substituting 
%
\begin{equation}
y(x_n, \mathbf{w}) = \sum_{j = 0}^{M} w_j x_n^j
\end{equation}
%
gives
%
\begin{equation}
0 = \sum_{n = 1}^{N} x_n^i \left( \sum_{j = 0}^{M} w_j x_n^j - t_n \right) + \lambda w_i.
\end{equation}
%
Therefore,
%
\begin{equation}
\sum_{j = 0}^{M} \tilde{A}_{ij} w_j = T_i
\end{equation}
%
where
%
\begin{equation}
\begin{aligned}
\tilde{A}_{ij} &= \sum_{n = 1}^{N} x_n^{i + j} + \lambda \delta_{ij}, \\
T_i &= \sum_{n = 1}^{N} x_n^i t_n.
\end{aligned}
\end{equation}


\subsection{}
\label{1.3}
Let $a$, $o$ and $l$ be the events where an apple, orange and lime are selected respectively.
The probability that an apple is selected is given by
%
\begin{equation}
p(a) = p(a | r) p(r) + p(a | b) p(b) + p(a | g) p(g).
\end{equation}
%
Substituting $p(a | r) = \frac{3}{10}$, $p(r) = \frac{1}{5}$, $p(a | g) = \frac{1}{2}$, $p(r) = \frac{1}{5}$, $p(a | g) = \frac{3}{10}$ and $p(g) = \frac{3}{5}$ gives
%
\begin{equation}
p(a) = \frac{17}{50}.
\end{equation}
%

If an orange is selected, the probability that it came from the geen box is given by
%
\begin{equation}
p(g | o) = \frac{p(g, o)}{p(o)}.
\end{equation}
%
Here,
%
\begin{equation}
\begin{aligned}
p(g, o) &= p(o | g) p(g), \\
p(o) & = p(o | r) p(r) + p(o | b) p(b) + p(o | g) p(g).
\end{aligned}
\end{equation}
%
Substituting $p(o | r) = \frac{2}{5}$, $p(r) = \frac{1}{5}$, $p(o | b) = \frac{1}{2}$, $p(b) = \frac{1}{5}$, $p(o | g) = \frac{3}{10}$ and $p(g) = \frac{3}{5}$ gives $p(g, o) = \frac{9}{50}$ and $p(o) = \frac{9}{25}$.
%
Therefore,
\begin{equation}
p(g | o) = \frac{1}{2}.
\end{equation}


\subsection{}
\label{1.4}
Let 
%
\begin{equation}
x = g(y)
\end{equation}
%
and $\hat{x}$ and $\hat{y}$ be the locations of the maximum of $p_x(x)$ and $p_y(y)$ respcetively.
Let us assume that there exists $\epsilon > 0$ such that $g'(y) \neq 0$ for $\left| y - \hat{y} \right| < \epsilon$.
Then, differentiating both sides of the transoformation
%
\begin{equation}
p_y(y) = p_x \left( g(y) \right) \abs{g'(y)}
\end{equation}
%
and substituting $y = \hat{y}$ gives
%
\begin{equation}
0 = g'(\hat{y}) p_x' \left( g \left( \hat{y} \right) \right) + p_x \left( g \left( \hat{y} \right) \right) g'' \left( \hat{y} \right).
\end{equation}
%
Therefore, in general,
%
\begin{equation}
\hat{x} \neq g \left( \hat{y} \right).
\end{equation}
%

Here, let us assume that 
%
\begin{equation}
g(y) = a y + b.
\end{equation}
%
Then, differentiating both sides of the transformation and substituting $y = \hat{y}$ gives
%
\begin{equation}
0 = p_x' \left( g \left( \hat{y} \right) \right).
\end{equation}
%
Therefore, 
%
\begin{equation}
\hat{x} = g \left( \hat{y} \right).
\end{equation}
%


\subsection{}
\label{1.5}
By the definition, 
%
\begin{equation}
{\rm var} f(x) = {\rm E} \left( f(x) - {\rm E} f(x) \right) ^ 2.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
{\rm E} \left( \left( f(x) \right) ^ 2 - 2 f(x) {\rm E} f(x) + \left( {\rm E} f(x) \right) ^ 2 \right) = {\rm E} \left( f(x) \right) ^ 2 - \left( {\rm E} f(x) \right) ^ 2.
\end{equation}
%
Therefore, 
%
\begin{equation}
{\rm var} f(x) = {\rm E} \left( f(x) \right) ^ 2 - \left( {\rm E} f(x) \right) ^ 2.
\end{equation}
%


\subsection{}
\label{1.6}
By the definition,
%
\begin{equation}
{\rm cov} (x, y) = {\rm E} \left( \left( x - {\rm E} x \right) \left( y - {\rm E} y\right) \right).
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
{\rm E} x y - {\rm E} \left( x {\rm E} y \right) - {\rm E} \left( y {\rm E} x \right) + {\rm E} \left( {\rm E} x {\rm E} y \right) = {\rm E} x y - {\rm E} x {\rm E} y.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\int x y p(x, y) dx dy - \int x p(x) dx \int y p(y) dy.
\end{equation}
%
If $x$ and $y$ are independent, by the definition,
%
\begin{equation}
f(x, y) = f(x) f(y).
\end{equation}
%
Then,
%
\begin{equation}
\int x y p(x, y) dx dy = \int p(x) dx \int p(y) dy.
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm cov} (x, y) = 0.
\end{equation}
%


\subsection{}
\label{1.7}
Let
%
\begin{equation}
I = \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2 \sigma ^ 2} x ^ 2 \right) dx.
\end{equation}
%
Then
%
\begin{equation}
I ^ 2 = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2 \sigma ^ 2} \left( x ^ 2 + y ^ 2 \right) \right) dx dy.
\end{equation}
%
By the transformation from Cartesian coordinates $(x, y)$ to polar coordinates $(r, \theta)$, the right hand side can be written as
%
\begin{equation}
\int_{0}^{\infty} \int_{0}^{2 \pi} \exp \left( - \frac{1}{2 \sigma ^ 2} r ^ 2 \right) 
\begin{vmatrix}
\cos \theta & - r \sin \theta \\
\sin \theta &  r \cos \theta \\
\end{vmatrix}
dr d\theta
= 2 \pi \int_{0}^{\infty} \exp \left( - \frac{1}{2 \sigma ^ 2} r ^ 2 \right) r dr.
\end{equation}
%
By the transformation $s = \frac{r}{\sigma}$, the right hand side can be written as
%
\begin{equation}
2 \pi \sigma ^ 2 \int_{0}^{\infty} \exp \left( - \frac{1}{2} s ^ 2 \right) s ds = 2 \pi \sigma ^ 2 \left[ - \exp \left( - \frac{1}{2} s ^ 2 \right) \right]_0^\infty.
\end{equation}
%
Therefore, 
%
\begin{equation}
I = \left( 2 \pi \sigma ^ 2 \right) ^ \frac{1}{2}. 
\end{equation}
%

By the definition,
%
\begin{equation}
\mathcal{N} \left( x | \mu, \sigma ^ 2 \right) = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \exp \left( - \frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right).
\end{equation}
%
Then
%
\begin{equation}
\int_{- \infty}^{\infty} \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right) dx.
\end{equation}
%
By the transformation $t = x - \mu$, the right hand side can be written as 
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2 \sigma ^ 2} t ^ 2 \right) dt = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} I.
\end{equation}
%
Therefore,
%
\begin{equation}
\int_{- \infty}^{\infty} \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx = 1.
\end{equation}
%


\subsection{}
\label{1.8}
Let $x$ be a variable under the Gaussian distribution with mean $\mu$ and variance $\sigma ^ 2$.
Then
%
\begin{equation}
{\rm E} x = \int_{-\infty}^{\infty} x \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx.
\end{equation}
%
By the definition, the right hand side can be written as
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} x \exp \left( -\frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right) dx.
\end{equation}
%
By the transformation $y = x - \mu$, it can be written as 
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} (y + \mu) \exp \left( -\frac{1}{2 \sigma ^ 2}y ^ 2 \right) dy.
\end{equation}
%
Since 
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} y \exp \left( -\frac{1}{2 \sigma ^ 2}y ^ 2 \right) dy = 0,
\end{equation}
%
and
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} \mu \exp \left( -\frac{1}{2 \sigma ^ 2}y ^ 2 \right) dy = \mu \int_{- \infty}^{\infty} \mathcal{N} \left( y | \mu, \sigma ^ 2 \right) dy,
\end{equation}
%
we have
%
\begin{equation}
{\rm E} x = \mu.
\end{equation}
%

By the definition,  
%
\begin{equation}
\int_{- \infty}^{\infty} \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx = 1
\end{equation}
%
can be written as
%
\begin{equation}
\left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} \exp \left( - \frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right) dx = 1.
\end{equation}
%
Differentiating both sides with respect to $\sigma ^ 2$ gives 
%
\begin{equation}
\begin{aligned}
\left( 2 \pi \right) ^ {-\frac{1}{2}} \left( - \frac{1}{2} \right) \left( \sigma ^ 2 \right) ^ {- \frac{3}{2}} \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right) dx \\
+ \left( 2 \pi \sigma ^ 2 \right) ^ {-\frac{1}{2}} \int_{-\infty}^{\infty} \frac{1}{2} \left( \sigma ^ 2 \right) ^ {- 2} (x - \mu) ^ 2 \exp \left( - \frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right) dx = 0.
\end{aligned}
\end{equation}
%
The left hand side can be written as
%
\begin{equation}
\begin{aligned}
- \frac{1}{2} \left( \sigma ^ 2 \right) ^ {- 1} \int_{- \infty}^{\infty} \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx + \frac{1}{2} \left( \sigma ^ 2 \right) ^ {- 2} \int_{- \infty} ^ {\infty} (x - \mu) ^ 2 \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx \\
= - \frac{1}{2} \left( \sigma ^ 2 \right) ^ {- 1} + \frac{1}{2} \left( \sigma ^ 2 \right) ^ {- 2} {\rm var} x.
\end{aligned}
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm var} x = \sigma ^ 2.
\end{equation}
%


\subsection{}
\label{1.9}
Let
%
\begin{equation}
\mathcal{N} \left( x | \mu, \sigma ^ 2 \right) = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \exp \left( - \frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right).
\end{equation}
%
Setting its derivative with respect to $x$ to zero gives
%
\begin{equation}
0 = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \left( - \frac{1}{\sigma ^ 2} (x - \mu) \right) \exp \left( - \frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right).
\end{equation}
%
Therefore, the mode is given by $\mu$.

Similarly, let
%
\begin{equation}
\mathcal{N} \left( \mathbf{x} | \bm{\mu}, \bm{\Sigma} \right) = \left( 2 \pi \right) ^ {- \frac{D}{2}} |\bm{\Sigma}| ^ {- \frac{1}{2}} \exp \left( - \frac{1}{2} (\mathbf{x} - \bm{\mu}) ^ \intercal \bm{\Sigma} ^ {- 1} (\mathbf{x} - \bm{\mu}) \right).
\end{equation}
%
Setting its derivative with respect to $\mathbf{x}$ to zero gives
%
\begin{equation}
\mathbf{0} = - \left( 2 \pi \right) ^ {- \frac{D}{2}} |\bm{\Sigma}| ^ {- \frac{1}{2}} \left( \bm{\Sigma} ^ {- 1} + \left( \bm{\Sigma} ^ {- 1} \right) ^ \intercal \right) (\mathbf{x} - \bm{\mu}) \exp \left( - \frac{1}{2} (\mathbf{x} - \bm{\mu}) ^ \intercal \bm{\Sigma} ^ {- 1} (\mathbf{x} - \bm{\mu}) \right).
\end{equation}
%
Therefore, the mode is given by $\bm{\mu}$.


\subsection{}
\label{1.10}
By the definition,
%
\begin{equation}
{\rm E} (x + y) = \int \int (x + y) p(x, y) dx dy.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
\int x \left( \int p(x, y) dy \right) dx + \int y \left( \int p(x, y) dx \right) dy = \int x p(x) dx + \int y p(y) dy.
\end{equation}
%
By the definition, the right hand side can be written as 
%
\begin{equation}
{\rm E} x + {\rm E} y.
\end{equation}
%
Therefore, 
%
\begin{equation}
{\rm E} (x + y) = {\rm E} x + {\rm E} y.
\end{equation}
%

Similarly, by the definition,
%
\begin{equation}
{\rm var} (x + y) = {\rm E} \left( x + y - {\rm E} (x + y) \right) ^ 2
\end{equation}
%
By the result above and the definition, the right hand side can be written as 
%
\begin{equation}
\begin{aligned}
{\rm E} \left( x - {\rm E} x \right) ^ 2 + 2 {\rm E} \left( \left( x - {\rm E} x \right) \left( y - {\rm E} y \right) \right) + {\rm E} \left( y - {\rm E} y \right) ^ 2 \\
= {\rm var} x + 2 {\rm cov} (x, y) + {\rm var} y.
\end{aligned}
\end{equation}
%
If $x$ and $y$ are independent, then 
%
\begin{equation}
{\rm cov} (x, y) = 0,
\end{equation}

%
by \ref{1.6}. Therefore,
%
\begin{equation}
{\rm var} (x + y) = {\rm var} x + {\rm var} y.
\end{equation}
%


\subsection{}
\label{1.11}
Let
%
\begin{equation}
\ln p \left( \mathbf{x} | \mu, \sigma ^ 2 \right) = - \frac{N}{2} \ln \left( 2 \pi \sigma ^ 2 \right) - \frac{1}{2 \sigma ^ 2} \sum_{n = 1}^{N} (x_n - \mu) ^ 2.
\end{equation}
%
To maximise it with respect to $\mu$ and $\sigma ^ 2$, setting the partial derivatives to zero gives
%
\begin{equation}
\begin{aligned}
0 &= \frac{1}{\sigma ^ 2} \sum_{n = 1}^{N} (x_n - \mu), \\
0 &= - \frac{N}{2 \sigma ^ 2} + \frac{1}{2 \left( \sigma ^ 2 \right) ^ 2} \sum_{n = 1}^{N} (x_n - \mu) ^ 2.
\end{aligned}
\end{equation}
%
Therefore,
%
\begin{equation}
\begin{aligned}
\mu_{\rm ML} &= \frac{1}{N} \sum_{n = 1}^{N} x_n, \\
\sigma_{\rm ML} ^ 2 &= \frac{1}{N} \sum_{n = 1}^{N} (x_n - \mu_{\rm ML}) ^ 2.
\end{aligned}
\end{equation}
%


\subsection{}
\label{1.12}
Let $x_m$ and $x_n$ be independent variables. 
Then
%
\begin{equation}
{\rm E} x_m x_n = {\rm E} x_m {\rm E} x_n.
\end{equation}
%
If they are samples from the Gaussian distribution with mean $\mu$ and variance $\sigma ^ 2$, the right hand side is given by $\mu ^ 2$.
On the other hand, by the definition, 
%
\begin{equation}
{\rm E} x_n ^ 2 =  {\rm var} x_n + \left( {\rm E} x_n \right) ^ 2.
\end{equation}
%
If $x_n$ is a sample from the Gaussian distribution with mean $\mu$ and variance $\sigma ^ 2$, the right hand side is given by $\sigma ^ 2 + \mu ^ 2$.
Therefore,
%
\begin{equation}
{\rm E} x_m x_n = \mu ^ 2 + \delta_{mn} \sigma ^ 2.
\end{equation}
%

Here, since 
%
\begin{equation}
\mu_{\rm ML} = \frac{1}{N} \sum_{n = 1}^{N} x_n,
\end{equation}
%
we have
%
\begin{equation}
{\rm E} \mu_{\rm ML} = \frac{1}{N} \sum_{n = 1}^{N} {\rm E} x_n.
\end{equation}
%
Therefore, 
%
\begin{equation}
{\rm E} \mu_{\rm ML} = \mu.
\end{equation}
%
Similarly, since 
%
\begin{equation}
\sigma_{\rm ML}^2 = \frac{1}{N} \sum_{n = 1}^{N} \left( x_n - \mu_{\rm ML} \right) ^ 2,
\end{equation}
%
we have 
%
\begin{equation}
{\rm E} \sigma_{\rm ML}^2 = \frac{1}{N} \sum_{n = 1}^{N} {\rm E} \left( x_n - \mu_{\rm ML} \right) ^ 2.
\end{equation}
%
The right hand side can be writen as 
%
\begin{equation}
\frac{1}{N} \sum_{n = 1}^{N} {\rm E} \left( x_n ^ 2 - 2 \mu_{\rm ML} x_n + \mu_{\rm ML} ^ 2 \right) = \frac{1}{N} \sum_{n = 1}^{N} {\rm E} x_n ^ 2 - \frac{2}{N} {\rm E} \left( \mu_{\rm ML} \left( \sum_{n = 1}^{N} x_n \right) \right) + {\rm E} \mu_{\rm ML} ^ 2.
\end{equation}
%
The first term of the right hand side can be written as 
%
\begin{equation}
\frac{1}{N} \sum_{n = 1}^{N} \left( \mu ^ 2 + \sigma ^ 2 \right) = \mu ^ 2 + \sigma ^ 2,
\end{equation}
%
while the second and third terms can be writen as
%
\begin{equation}
- 2 {\rm E} \mu_{\rm ML} ^ 2 + {\rm E} \mu_{\rm ML} ^ 2 = - {\rm E} \mu_{\rm ML} ^ 2.
\end{equation}
%
Here, 
%
\begin{equation}
{\rm E} \mu_{\rm ML} ^ 2 = {\rm E} \left( \frac{1}{N} \sum_{n = 1}^{N} x_n \right) ^ 2.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
\frac{1}{N ^ 2} \sum_{n = 1}^{N} {\rm E} x_n ^ 2 + \frac{2}{N ^ 2} \sum_{1 \leq m < n \leq N} {\rm E} x_m x_n = \frac{1}{N} (\mu ^ 2 + \sigma ^ 2) + \frac{N - 1}{N} \mu ^ 2.
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm E} \mu_{\rm ML} ^ 2 = \mu ^ 2 + \frac{1}{N} \sigma ^ 2.
\end{equation}
%
Thus,
%
\begin{equation}
{\rm E} \sigma_{\rm ML}^2 = \frac{N - 1}{N} \sigma ^ 2.
\end{equation}
%


\subsection{}
\label{1.13}
Let $\{ x_n \}$ be a set of variables whose mean is $\mu$ and variance is $\sigma ^ 2$.
Then
%
\begin{equation}
{\rm E} \left( \frac{1}{N} \sum_{n = 1}^{N} \left( x_n - \mu \right) ^ 2 \right) = \frac{1}{N} \sum_{n = 1}^{N} {\rm E} \left( x_n - \mu \right) ^ 2.
\end{equation}
%
The right hand side can be writen as 
%
\begin{equation}
\frac{1}{N} \sum_{n = 1}^{N} {\rm E} \left( x_n ^ 2 - 2 \mu x_n + \mu ^ 2 \right) = \frac{1}{N} \sum_{n = 1}^{N} {\rm E} x_n ^ 2 - \frac{2 \mu}{N}  \sum_{n = 1}^{N} {\rm E} x_n + \mu ^ 2.
\end{equation}
%
The first term of the right hand side can be written as 
%
\begin{equation}
\frac{1}{N} \sum_{n = 1}^{N} \left( \mu ^ 2 + \sigma ^ 2 \right) = \mu ^ 2 + \sigma ^ 2,
\end{equation}
%
while the second term can be writen as
%
\begin{equation}
- \frac{2 \mu}{N} \sum_{n = 1}^{N} \mu = - 2 \mu ^ 2.
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm E} \left( \frac{1}{N} \sum_{n = 1}^{N} \left( x_n - \mu \right) ^ 2 \right) = \sigma ^2.
\end{equation}
%


\subsection{}
\label{1.14}
Let
%
\begin{equation}
\begin{aligned}
w_{ij}^{\rm S} &= \frac{1}{2} (w_{ij} + w_{ji}), \\
w_{ij}^{\rm A} &= \frac{1}{2} (w_{ij} - w_{ji}).
\end{aligned}
\end{equation}
%
Then
%
\begin{equation}
\begin{aligned}
w_{ij} &= w_{ij}^{\rm S} + w_{ij}^{\rm A}, \\
w_{ij}^{\rm S} &= w_{ji}^{\rm S}, \\
w_{ij}^{\rm A} &= - w_{ji}^{\rm A}.
\end{aligned}
\end{equation}
%
Here,
\begin{equation}
\sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij}^{\rm A} x_i x_j = \frac{1}{2} \sum_{i = 1}^{D} \sum_{j = 1}^{D} (w_{ij} - w_{ji}) x_i x_j.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\frac{1}{2} \left( \sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij} x_i x_j - \sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ji} x_i x_j \right) = 0.
\end{equation}
%
Therefore, 
%
\begin{equation}
\sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij}^{\rm A} x_i x_j = 0.
\end{equation}
%

Additionally,
%
\begin{equation}
\sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij} x_i x_j = \sum_{i = 1}^{D} \sum_{j = 1}^{D} \left( w_{ij}^{\rm S} + w_{ij}^{\rm A} \right) x_i x_j.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij}^{\rm S} x_i x_j + \sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij}^{\rm A} x_i x_j = \sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij}^{\rm S} x_i x_j,
\end{equation}
%
where the result above is used. 
Therefore,
%
\begin{equation}
\sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij} x_i x_j = \sum_{i = 1}^{D} \sum_{j = 1}^{D} w_{ij}^{\rm S} x_i x_j.
\end{equation}
%

Finally, since the matrix $w_{ij}^{\rm S}$ is $D \times D$ symmetric matrix, its number of independent parameters is $\frac{D (D + 1)}{2}$.


\subsection{(Incomplete)}
\label{1.15}


\subsection{(Incomplete)}
\label{1.16}


\subsection{}
\label{1.17}
Let
%
\begin{equation}
\Gamma(x) = \int_{0}^{\infty} u ^ {x - 1} \exp(- u) du.
\end{equation}
%
Then
%
\begin{equation}
\Gamma(x + 1) = \int_{0}^{\infty} u ^ {x} \exp(- u) du.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
\left[ - u ^ x \exp(- u) \right]_{u = 0}^{u = \infty} + \int_{0}^{\infty} x u ^ {x - 1} \exp(- u) du = x \Gamma(x).
\end{equation}
%
Therefore,
%
\begin{equation}
\Gamma( x + 1) = x \Gamma(x).
\end{equation}
%

Since
%
\begin{equation}
\Gamma(1) = \int_{0}^{1} \exp(- u) du,
\end{equation}
%
and the right hand side can be written as $1$,
%
\begin{equation}
\Gamma(1) = 0!.
\end{equation}
%
For a positive integer $x$, let us assume that
%
\begin{equation}
\Gamma(x) = (x - 1)!.
\end{equation}
%
Then,
%
\begin{equation}
\Gamma(x + 1) = x \Gamma(x),
\end{equation}
%
where the right hand side can be written as
%
\begin{equation}
x (x - 1)! = x!.
\end{equation}
%
Therefore,
%
\begin{equation}
\Gamma(x + 1) = x!.
\end{equation}
%
Thus, the assumption is proved by induction on $x$.


\subsection{}
\label{1.18}
Let us consider the transformation from Cartesian to polar coordinates
%
\begin{equation}
\prod_{i = 1}^{D} \int_{- \infty}^{\infty} \exp \left( - x_i ^ 2 \right) dx_i = S_D \int_{0}^{\infty} \exp \left( - r ^ 2 \right) r ^ {D - 1} dr,
\end{equation}
%
where $S_D$ is the surface area of a sphere of unit raidus in $D$ dimensions. 
By \ref{1.7}, the left hand side can be written as $\pi ^ \frac{D}{2}$. By the transformation $s = r ^ 2$, the right hand side can be written as
%
\begin{equation}
\frac{S_D}{2} \int_0^\infty \exp(- s) s ^ \frac{D - 1}{2} s ^ {- \frac{1}{2}} ds = \frac{S_D}{2} \Gamma \left( \frac{D}{2} \right).
\end{equation}
%
Therefore,
%
\begin{equation}
S_D = \frac{2 \pi ^ \frac{D}{2}}{\Gamma \left( \frac{D}{2} \right)}.
\end{equation}
%

Additionally, the volume of the sphere can can be written as
%
\begin{equation}
V_D = S_D \int_{0}^{1} r^{D - 1} dr.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
S_D \left[ \frac{r ^ D}{D} \right]_{r = 0}^{r = 1} = \frac{S_D}{D}.
\end{equation}
%
Therefore, 
%
\begin{equation}
V_D = \frac{S_D}{D}.
\end{equation}
%

Finally, the results above reduce to
%
\begin{equation}
\begin{aligned}
S_2 &= \frac{2 \pi}{\Gamma(1)}, \\
V_2 &= \frac{S_2}{2}. 
\end{aligned}
\end{equation}
%
Therefore, 
%
\begin{equation}
\begin{aligned}
S_2 &= 2 \pi, \\
V_2 &= \pi. 
\end{aligned}
\end{equation}
%
Similarly,
%
\begin{equation}
\begin{aligned}
S_3 &= \frac{2 \pi ^ \frac{3}{2}}{\Gamma \left( \frac{3}{2} \right)}, \\
V_3 &= \frac{S_3}{3}. 
\end{aligned}
\end{equation}
%
Therefore, 
%
\begin{equation}
\begin{aligned}
S_3 &= 4 \pi, \\
V_3 &= \frac{4}{3} \pi. 
\end{aligned}
\end{equation}
%


\subsection{}
\label{1.19}
The volume of a cube of side 2 in $D$ dimensions is $2 ^ D$. 
Therefore, the ratio of the volume of the cocentric sphere of radius 1 divided by the volume of the cube is given by
%
\begin{equation}
\frac{V_D}{2 ^ D} = \frac{\pi ^ \frac{D}{2}}{D 2 ^ {D - 1} \Gamma \left( \frac{D}{2} \right)},
\end{equation}
%
by \ref{1.18}.

Additionally, by Stering's formula
%
\begin{equation}
\Gamma(x + 1) \simeq (2 \pi) ^ \frac{1}{2} \exp(- x) x ^ {\frac{x + 1}{2}},
\end{equation}
%
the ratio can be approximated as
%
\begin{equation}
\frac{V_D}{2 ^ D} \simeq \frac{\pi ^ \frac{D}{2}}{D 2 ^ {D - 1} (2 \pi) ^ \frac{1}{2} \exp \left( 1 - \frac{D}{2} \right) \left( \frac{D}{2} - 1 \right) ^ \frac{D}{4}}.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
\frac{1}{2 e (2 \pi) ^ \frac{1}{2}} \frac{1}{D} \left( \frac{e ^ 2 \pi ^ 2}{ 8 D - 16} \right) ^ \frac{D}{4}.
\end{equation}
%
Therefore, the ratio goes to zero as $D \to \infty$.

Finally, the ratio of the distance from the center of the cube to one of the corners divided by the perpendicular distance to one of the sides is given by
%
\begin{equation}
\frac{\sqrt{\sum_{i = 1}^{D} 1 ^ 2}}{1} = \sqrt{D}.
\end{equation}
%
Therefore, the ration goes to $\infty$ as $D \to \infty$.


\subsection{}
\label{1.20}
For a vector $\mathbf{x}$ in $D$ dimensions, let 
%
\begin{equation}
p(\mathbf{x}) = (2 \pi \sigma ^ 2) ^ {- \frac{D}{2}} \exp \left( - \frac{\norm{\mathbf{x}} ^ 2}{2 \sigma ^2} \right).
\end{equation}
%
Integrating both sides from $\norm{\mathbf{x}} = r$ to $\norm{\mathbf{x}} = r + \epsilon$ gives
%
\begin{equation}
\int_{r \leq \norm{\mathbf{x}} \leq r + \epsilon} p(\mathbf{x}) d\mathbf{x} = \int_{r}^{r + \epsilon} \int (2 \pi \sigma ^ 2) ^ {- \frac{D}{2}} \exp \left( - \frac{{r'} ^ 2}{2 \sigma ^2} \right) J dr' d\bm{\phi},
\end{equation}
%
where $\bm{\phi}$ is the vector of the angular components of the polar corrdinate and $J$ is the Jacobian of the transformation from the Cartesian to polar coordinate.
For a sufficiently small $\epsilon$, the right hand side can be approximated as
%
\begin{equation}
\begin{aligned}
(2 \pi \sigma ^ 2) ^ {- \frac{D}{2}} \exp \left( - \frac{r ^ 2}{2 \sigma ^2} \right) \int_{r}^{r + \epsilon} \int J dr' d\bm{\phi} \\
= (2 \pi \sigma ^ 2) ^ {- \frac{D}{2}} \exp \left( - \frac{r ^ 2}{2 \sigma ^2} \right) \int_{r \leq \norm{\mathbf{x}} \leq r + \epsilon} d\mathbf{x}.
\end{aligned}
\end{equation}
%
Therefore,
%
\begin{equation}
\int_{r \leq \norm{\mathbf{x}} \leq r + \epsilon} p(\mathbf{x}) d\mathbf{x} \simeq p(r) \epsilon,
\end{equation}
%
where
%
\begin{equation}
p(r) = (2 \pi \sigma ^ 2) ^ {- \frac{D}{2}} S_D r ^ {D - 1} \exp \left( - \frac{r ^ 2}{2 \sigma ^2} \right),
\end{equation}
%
and $S_D$ is the surface area of a unit sphere in $D$ dimensions.

Secondly, to maximise $p(r)$, setting the derivative to zero gives
%
\begin{equation}
0 = (2 \pi \sigma ^ 2) ^ {- \frac{D}{2}} S_D \left( (D - 1) r ^ {D - 2} - \frac{r ^ D}{\sigma ^ 2} \right) \exp \left( - \frac{r ^ 2}{2 \sigma ^2} \right).
\end{equation}
%
Therefore, $p(r)$ is maximised at a sigle stationary point
%
\begin{equation}
\hat{r} = \sqrt{D - 1} \sigma.
\end{equation}
%

Thirdly, by the expression of $p(r)$ above,
%
\begin{equation}
\frac{p \left( \hat{r} + \epsilon \right)}{p \left( \hat{r} \right)} = \left( \frac{\hat{r} + \epsilon}{\hat{r}} \right) ^ {D - 1} \exp \left( - \frac{2 \hat{r} \epsilon + \epsilon ^ 2}{2 \sigma ^ 2} \right).
\end{equation}
%
Using the expression of $\hat{r}$ above, the right hand side can be written as
%
\begin{equation}
\begin{aligned}
\exp \left( (D - 1) \ln \left( 1 + \frac{\epsilon}{\hat{r}} \right) - \frac{2 \hat{r} \epsilon + \epsilon ^ 2}{2 \sigma ^ 2} \right) \\
= \exp \left( \frac{\hat{r} ^ 2}{\sigma ^ 2} \ln \left( 1 + \frac{\epsilon}{\hat{r}} \right) - \frac{2 \hat{r} \epsilon + \epsilon ^ 2}{2 \sigma ^ 2} \right).
\end{aligned}
\end{equation}
%
By the Taylor series
%
\begin{equation}
\ln (1 + x) = x - \frac{1}{2} x ^ 2 + o \left( x ^ 3 \right),
\end{equation}
%
the right hand side can be approximated as
%
\begin{equation}
\exp \left( \frac{\hat{r} ^ 2}{\sigma ^ 2} \left( \frac{\epsilon}{\hat{r}} - \frac{\epsilon ^ 2}{2 {\hat{r}} ^ 2} \right) - \frac{2 \hat{r} \epsilon + \epsilon ^ 2}{2 \sigma ^ 2} \right) = \exp \left( - \frac{\epsilon ^ 2}{\sigma ^ 2} \right).
\end{equation}
%
Therefore,
%
\begin{equation}
p \left( \hat{r} + \epsilon \right) \simeq p \left( \hat{r} \right) \exp \left( - \frac{\epsilon ^ 2}{\sigma ^ 2} \right).
\end{equation}
%

Finally, let a vector of length $\hat{r}$ be $\hat{\mathbf{r}}$.
Then, by the definition of $p(\mathbf{x})$,
%
\begin{equation}
\frac{p(\mathbf{0})}{p \left( \hat{\mathbf{r}} \right)} = \exp \left( \frac{\hat{r} ^ 2}{2 \sigma ^2} \right).
\end{equation}
%
Substituting the expression of $\hat{r}$ above, the right hand side can be written as $\exp \left( \frac{D - 1}{2} \right)$.
Therefore,
%
\begin{equation}
\frac{p(\mathbf{0})}{p \left( \hat{\mathbf{r}} \right)} = \exp \left( \frac{D - 1}{2} \right).
\end{equation}
%


\subsection{}
\label{1.21}
If $0 \leq a \leq b$, then
%
\begin{equation}
0 \leq a (b - a).
\end{equation}
%
Therefore,
%
\begin{equation}
a \leq (ab) ^ \frac{1}{2}.
\end{equation}
%

For a two-class classification problem of $\mathbf{x}$, let the classes be $\mathcal{C}_1$ and $\mathcal{C}_2$ and let the decision regions be $\mathcal{R}_1$ and $\mathcal{R}_2$.
Let us choose the decision regions to minimise the probability of misclassification.
Then,
%
\begin{equation}
p(\mathbf{x}, \mathcal{C}_1) > p(\mathbf{x}, \mathcal{C}_2) \Rightarrow \mathbf{x} \in \mathcal{C}_1, 
\end{equation}
%
and
%
\begin{equation}
p(\mathbf{x}, \mathcal{C}_2) > p(\mathbf{x}, \mathcal{C}_1) \Rightarrow \mathbf{x} \in \mathcal{C}_2.
\end{equation}
%
Then, using the inequality above,
%
\begin{equation}
\int_{\mathcal{R}_1} p(\mathbf{x}, \mathcal{C}_2) d\mathbf{x} \leq \int_{\mathcal{R}_1} \left( p(\mathbf{x}, \mathcal{C}_1) p(\mathbf{x}, \mathcal{C}_2) \right) ^ \frac{1}{2} d\mathbf{x},
\end{equation}
%
and
%
\begin{equation}
\int_{\mathcal{R}_2} p(\mathbf{x}, \mathcal{C}_1) d\mathbf{x} \leq \int_{\mathcal{R}_2} \left( p(\mathbf{x}, \mathcal{C}_1) p(\mathbf{x}, \mathcal{C}_2) \right) ^ \frac{1}{2} d\mathbf{x}.
\end{equation}
%
Therefore,
%
\begin{equation}
\int_{\mathcal{R}_1} p(\mathbf{x}, \mathcal{C}_2) d\mathbf{x} + \int_{\mathcal{R}_2} p(\mathbf{x}, \mathcal{C}_1) d\mathbf{x} \leq \int \left( p(\mathbf{x}, \mathcal{C}_1) p(\mathbf{x}, \mathcal{C}_2) \right) ^ \frac{1}{2} d\mathbf{x}.
\end{equation}
%


\subsection{}
\label{1.22}
Let
%
\begin{equation}
{\rm E} L = \sum_{k} \sum_{j} \int_{\mathcal{R}_j} L_{kj} p(\mathbf{x}, \mathcal{C}_k) d\mathbf{x}.
\end{equation}
%
If
%
\begin{equation}
L_{kj} = 1 - \delta_{kj},
\end{equation}
%
then the right hand side can be written as
%
\begin{equation}
\sum_{k} \sum_{j} \int_{\mathcal{R}_j} \left( p(\mathbf{x}, \mathcal{C}_k) - p(\mathbf{x}, \mathcal{C}_j) \right)  d\mathbf{x} = \sum_{j} \int_{\mathcal{R}_j} \left( \sum_{k} p(\mathbf{x}, \mathcal{C}_k) - p(\mathbf{x}, \mathcal{C}_j) \right)  d\mathbf{x}.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\sum_{j} \int_{\mathcal{R}_j} \left( p(\mathbf{x}) - p(\mathbf{x}, \mathcal{C}_j) \right)  d\mathbf{x} = 1 - \sum_{j} \int_{\mathcal{R}_j} p(\mathbf{x}, \mathcal{C}_j) d\mathbf{x}.
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm E} L = 1 - \sum_{j} \int_{\mathcal{R}_j} p(\mathcal{C}_j | \mathbf{x}) p(\mathbf{x}) d\mathbf{x}.
\end{equation}
%
Thus, minimising ${\rm E} L$ reduces to choosing the criterion to maximise the posterior probatility $p(\mathcal{C}_j | \mathbf{x})$.


\subsection{}
\label{1.23}
Let
%
\begin{equation}
{\rm E} L = \sum_{k} \sum_{j} \int_{\mathcal{R}_j} L_{kj} p(\mathbf{x}, \mathcal{C}_k) d\mathbf{x}.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\sum_{j} \int_{\mathcal{R}_j} \sum_{k} L_{kj} p(\mathbf{x}, \mathcal{C}_k) d\mathbf{x} = \sum_{j} \int_{\mathcal{R}_j} \left( \sum_{k} L_{kj} p(\mathcal{C}_k | \mathbf{x}) \right) p(\mathbf{x}) d\mathbf{x}.
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm E} L = \sum_{j} \int_{\mathcal{R}_j} \left( \sum_{k} L_{kj} p(\mathcal{C}_k | \mathbf{x}) \right) p(\mathbf{x}) d\mathbf{x}.
\end{equation}
%
Thus, mimising ${\rm E} L$ reduces to choosing to minimise $\sum_{k} L_{kj} p(\mathcal{C}_k | \mathbf{x})$.


\subsection{(Incomplete)}
\label{1.24}


\subsection{}
\label{1.25}
Let
%
\begin{equation}
{\rm E} L\left( \mathbf{t}, \mathbf{y} (\mathbf{x}) \right) = \int \int \lVert \mathbf{y}(\mathbf{x}) - \mathbf{t} \rVert ^ 2 p(\mathbf{x}, \mathbf{t}) d\mathbf{x} d\mathbf{t}.
\end{equation}
%
Then
%
\begin{equation}
\frac{\delta {\rm E} L\left( \mathbf{t}, \mathbf{y} (\mathbf{x}) \right)}{\delta \mathbf{y} (\mathbf{x})} = 2 \int \left( \mathbf{y} (\mathbf{x}) - \mathbf{t} \right) p(\mathbf{x}, \mathbf{t}) d\mathbf{t}.
\end{equation}
%
To minimise ${\rm E} L\left( \mathbf{t}, \mathbf{y} (\mathbf{x}) \right)$, setting the left hand side to zero gives
%
\begin{equation}
\mathbf{0} = \int \left( \mathbf{y}(\mathbf{x}) - \mathbf{t} \right) p(\mathbf{t} | \mathbf{x}) d\mathbf{t}.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\mathbf{y}(\mathbf{x}) \int p(\mathbf{t} | \mathbf{x}) d\mathbf{t} - \int \mathbf{t} p(\mathbf{t} | \mathbf{x}) d\mathbf{t} = \mathbf{y}(\mathbf{x}) - {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}).
\end{equation}
%
Thus,
%
\begin{equation}
\mathbf{y}(\mathbf{x}) = {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}).
\end{equation}
%

Finally, for a single target variable $t$, it reduces to 
%
\begin{equation}
\mathbf{y}(\mathbf{x}) = {\rm E}_t (t | \mathbf{x}).
\end{equation}
%


\subsection{}
\label{1.26}
Let
%
\begin{equation}
{\rm E} L\left( \mathbf{t}, \mathbf{y} (\mathbf{x}) \right) = \int \int \lVert \mathbf{y}(\mathbf{x}) - \mathbf{t} \rVert ^ 2 p(\mathbf{x}, \mathbf{t}) d\mathbf{x} d\mathbf{t}.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\begin{aligned}
&\int \int \lVert \mathbf{y}(\mathbf{x}) - {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) + {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) - \mathbf{t} \rVert ^ 2 p(\mathbf{x}, \mathbf{t}) d\mathbf{x} d\mathbf{t} \\ 
&= \int \int \lVert \mathbf{y}(\mathbf{x}) - {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) \rVert ^ 2 p(\mathbf{x}, \mathbf{t}) d\mathbf{x} d\mathbf{t} \\
&+ 2 \int \int \left( \mathbf{y}(\mathbf{x}) - {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) \right) ^ \intercal \left( {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) - \mathbf{t} \right) p(\mathbf{x}, \mathbf{t}) d\mathbf{x} d\mathbf{t} \\
&+ \int \int \lVert {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) - \mathbf{t} \rVert ^ 2 p(\mathbf{x}, \mathbf{t}) d\mathbf{x} d\mathbf{t}.
\end{aligned}
\end{equation}
%
Let us look at each term of the right hand side.
The first term can be written as
%
\begin{equation}
\int \lVert \mathbf{y}(\mathbf{x}) - {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) \rVert ^ 2 \left( \int p(\mathbf{x}, \mathbf{t}) d\mathbf{t} \right) d\mathbf{x} = \int \lVert \mathbf{y}(\mathbf{x}) - {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) \rVert ^ 2 p(\mathbf{x}) d\mathbf{x}.
\end{equation}
%
The second term can be written as
%
\begin{equation}
2 \int \left( \mathbf{y}(\mathbf{x}) - {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) \right) ^ \intercal \left( \int \left( {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) - \mathbf{t} \right) p(\mathbf{t} | \mathbf{x}) d\mathbf{t} \right) p(\mathbf{x}) d\mathbf{x}.
\end{equation}
%
Since
%
\begin{equation}
\begin{aligned}
\int {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) p(\mathbf{t} | \mathbf{x}) d\mathbf{t} &= {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) \frac{\int p(\mathbf{x}, \mathbf{t}) d\mathbf{t}}{p(\mathbf{x})}, \\
\int \mathbf{t} p(\mathbf{t} | \mathbf{x}) d\mathbf{t} &= {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}),
\end{aligned}
\end{equation}
%
the second term is zero.
The third term can be written as
%
\begin{equation}
\int \left( \int \lVert {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) - \mathbf{t} \rVert ^ 2 p(\mathbf{t} | \mathbf{x}) d\mathbf{t} \right) p(\mathbf{x}) d\mathbf{x} = \int {\rm var}_{\mathbf{t}} (\mathbf{t} | \mathbf{x}) p(\mathbf{x}) d\mathbf{x}.
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm E} L\left( \mathbf{t}, \mathbf{y} (\mathbf{x}) \right) = \int \lVert \mathbf{y}(\mathbf{x}) - {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}) \rVert ^ 2 p(\mathbf{x}) d\mathbf{x} + \int {\rm var}_{\mathbf{t}} (\mathbf{t} | \mathbf{x}) p(\mathbf{x}) d\mathbf{x}.
\end{equation}
%
Thus, ${\rm E} L\left( \mathbf{t}, \mathbf{y} (\mathbf{x}) \right)$ is mimimised if
%
\begin{equation}
\mathbf{y} (\mathbf{x}) = {\rm E}_\mathbf{t} (\mathbf{t} | \mathbf{x}).
\end{equation}
%.


\subsection{}
\label{1.27}
Let
%
\begin{equation}
{\rm E} L_q = \int \int \abs{y(\mathbf{x}) - t} ^ q p(\mathbf{x}, t) d\mathbf{x} dt.
\end{equation}
%
Then
%
\begin{equation}
\frac{\delta {\rm E} L_q}{\delta y (\mathbf{x})} = \int q \abs{y(\mathbf{x}) - t} ^ {q - 1} {\rm sign} (y(\mathbf{x}) - t) p(\mathbf{x}, t) dt.
\end{equation}
%
To minimise ${\rm E} L_q$, setting the left hand side to zero gives
%
\begin{equation}
0 = \int \abs{y(\mathbf{x}) - t} ^ {q - 1} {\rm sign} (y(\mathbf{x}) - t) p(t | \mathbf{x}) dt.
\end{equation}
%
This is the condition that $y(\mathbf{x})$ must satisfy in order to minimise ${\rm E} L_q$.

If $q = 1$, the condition can be written as
%
\begin{equation}
0 = \int_{y(\mathbf{x})}^{\infty} p(t | \mathbf{x}) dt - \int_{- \infty}^{y(\mathbf{x})} p(t | \mathbf{x}) dt.
\end{equation}
%
Therefore, $y(\mathbf{x})$ is given by the conditional median.


\subsection{}
\label{1.28}
Let us assume that
%
\begin{equation}
p(x, y) = p(x) p(y) \Rightarrow h(x, y) = h(x) + h(y).
\end{equation}
%
Let $h(p)$ be a function to relate $h$ and $p$.
Then
%
\begin{equation}
h \left( p ^ 2 \right) = h(p) + h(p).
\end{equation}
%
Therefore,
%
\begin{equation}
h \left( p ^ 2 \right) = 2 h(p).
\end{equation}
%
Let us assume that, for a positive integer $n$,
%
\begin{equation}
h \left( p ^ n \right) = n h(p).
\end{equation}
%
Then, by the first assumption,
%
\begin{equation}
h \left( p ^ {n + 1} \right) = h \left( p ^ n \right) + h(p).
\end{equation}
%
Therefore,
%
\begin{equation}
h \left( p ^ {n + 1} \right) = (n + 1) h(p).
\end{equation}
%
Thus, the second assumption is proved by induction on $n$.

Additionally, for positive integers $m$ and $n$, 
%
\begin{equation}
h \left( p ^ n \right) = h \left( p ^ {\frac{n}{m} m} \right).
\end{equation}
%
By the second assumption, the left hand side can be written as $n h(p)$.
By the first assumption, the right hand side can be written as $m h \left( p ^ \frac{n}{m} \right)$.
Therefore,
%
\begin{equation}
h \left( p ^ \frac{n}{m} \right) = \frac{n}{m} h(p).
\end{equation}
%

Finally, by the continuity, for a positive real number $a$,
%
\begin{equation}
h \left( p ^a \right) = a h(p).
\end{equation}
%
Differentiating both sides with respect to $a$ and substituting $a = 1$ gives
%
\begin{equation}
(p \ln p) h' (p) = h(p).
\end{equation}
%
Therefore,
%
\begin{equation}
\int \frac{h'(p)}{h(p)} dp = \int \frac{1}{p \ln p} dp + C,
\end{equation}
%
where $C$ is a constant.
Ignorting the constants, the left hand side can be written as $\ln h(p)$ and the right hand side can be written as $\ln (\ln p)$.
Thus,
%
\begin{equation}
h(p) \propto \ln p.
\end{equation}
%


\subsection{}
\label{1.29}
Let $x$ be an $M$-state discrete random variable.
Then, by the definition,
%
\begin{equation}
{\rm H} (x) = - \sum_{i = 1}^{M} p(x_i) \ln p(x_i),
\end{equation}
%
where
%
\begin{equation}
\sum_{i = 1}^{M} p(x_i) = 1.
\end{equation}
%
By Jensen's inequality,
%
\begin{equation}
\sum_{i = 1}^{M} p(x_i) \ln \frac{1}{p(x_i)} \leq \ln \left( \sum_{i = 1}^{M} 1 \right).
\end{equation}
Therefore,
%
\begin{equation}
{\rm H} (x) \leq \ln M.
\end{equation}
%


\subsection{}
\label{1.30}
Let 
%
\begin{equation}
\begin{aligned}
p(x) &= \mathcal{N} \left( x | \mu, \sigma ^2 \right), \\
q(x) &= \mathcal{N} \left( x | m, s ^2 \right).
\end{aligned}
\end{equation}
%
Then, by the definition,
%
\begin{equation}
{\rm KL} \left( p || q \right) = - \int p(x) \ln \frac{q(x)}{p(x)} dx.
\end{equation}
%
The right hand side can be written as 
%
\begin{equation}
\begin{aligned}
&- \int_{- \infty}^{\infty} p(x) \ln \frac{\left( 2 \pi s ^ 2 \right) ^ {- \frac{1}{2}} \exp \left( - \frac{(x - m) ^ 2}{2 s ^ 2} \right)}{\left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \exp \left( - \frac{(x - \mu) ^ 2}{2 \sigma ^ 2} \right)} dx \\
& = - \int_{- \infty}^{\infty} p(x) \left( -\frac{1}{2} \ln \frac{s ^ 2}{\sigma ^ 2} - \frac{(x - m) ^ 2}{2 s ^ 2} + \frac{(x - \mu) ^ 2}{2 \sigma ^ 2} \right) dx.
\end{aligned}
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\ln \frac{s}{\sigma} \int_{- \infty}^{\infty} p(x) dx + \frac{1}{2 s ^2} \int_{- \infty}^{\infty} (x - m) ^ 2 p(x) dx - \frac{1}{2 \sigma ^2} \int_{- \infty}^{\infty} (x - \mu) ^ 2 p(x) dx.
\end{equation}
%
The first term can be written as $\ln \frac{s}{\sigma}$.
The second term can be written as
%
\begin{equation}
\frac{1}{2 s ^2} \int_{- \infty}^{\infty} (x - \mu + \mu - m) ^ 2 p(x) dx = \frac{\sigma ^ 2 + (\mu - m) ^ 2}{2 s ^ 2}.
\end{equation}
%
The third term can be written as $- \frac{1}{2}$.
Therefore,
%
\begin{equation}
{\rm KL} \left( p || q \right) = \ln \frac{s}{\sigma} + \frac{\sigma ^ 2 + (\mu - m) ^ 2}{2 s ^ 2} - \frac{1}{2}. 
\end{equation}
%


\subsection{}
\label{1.31}
Let $\mathbf{x}$ and $\mathbf{y}$ be two variables.
Then, by the definition, 
%
\begin{equation}
\begin{aligned}
{\rm H}(\mathbf{x}) &= - \int p(\mathbf{x}) \ln p(\mathbf{x}) d\mathbf{x}, \\
{\rm H}(\mathbf{y}) &= - \int p(\mathbf{y}) \ln p(\mathbf{y}) d\mathbf{y}, \\
{\rm H}(\mathbf{x}, \mathbf{y}) &= - \int \int p(\mathbf{x}, \mathbf{y}) \ln p(\mathbf{x}, \mathbf{y}) d\mathbf{x} d\mathbf{y}.
\end{aligned}
\end{equation}
%
Note that
%
\begin{equation}
\begin{aligned}
{\rm H}(\mathbf{x}) &= - \int \left( \int p(\mathbf{x}, \mathbf{y}) d\mathbf{y} \right) \ln p(\mathbf{x}) d\mathbf{x}, \\
{\rm H}(\mathbf{y}) &= - \int \left( \int p(\mathbf{x}, \mathbf{y}) d\mathbf{x} \right) \ln p(\mathbf{y}) d\mathbf{y}.
\end{aligned}
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm H}(\mathbf{x}) + {\rm H}(\mathbf{y}) - {\rm H}(\mathbf{x}, \mathbf{y}) = - \int \int p(\mathbf{x}, \mathbf{y}) \ln \frac{p(\mathbf{x}) p(\mathbf{y})}{p(\mathbf{x}, \mathbf{y})} d\mathbf{x} d\mathbf{y}.
\end{equation}
%
Since
%
\begin{equation}
\int \int p(\mathbf{x}, \mathbf{y}) d\mathbf{x} d\mathbf{y} = 1,
\end{equation}
%
Jensen's inequality can be used to write that
%
\begin{equation}
- \int \int p(\mathbf{x}, \mathbf{y}) \ln \frac{p(\mathbf{x}) p(\mathbf{y})}{p(\mathbf{x}, \mathbf{y})} d\mathbf{x} d\mathbf{y} \geq - \ln \left( \int \int p(\mathbf{x}) p(\mathbf{y}) d\mathbf{x} d\mathbf{y} \right).
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
- \ln \left( \int p(\mathbf{x}) d\mathbf{x} \int  p(\mathbf{y}) d\mathbf{y} \right) = 0.
\end{equation}
%
Thus,
%
\begin{equation}
{\rm H}(\mathbf{x}, \mathbf{y}) \leq {\rm H}(\mathbf{x}) + {\rm H}(\mathbf{y}).
\end{equation}
%


\subsection{}
\label{1.32}
Let $\mathbf{x}$ be a vector of continuous variables and 
%
\begin{equation}
\mathbf{y} = \mathbf{A} \mathbf{x},
\end{equation}
%
where $\mathbf{A}$ is a nonsingular matrix.
By the definition, 
%
\begin{equation}
{\rm H}(\mathbf{y}) = - \int p_y(\mathbf{y}) \ln p_y(\mathbf{y}) d\mathbf{y}.
\end{equation}
%
By the transformation
%
\begin{equation}
p_y(\mathbf{y}) = p_x(\mathbf{A} \mathbf{x}) \abs{\det \mathbf{A} ^ {-1}},
\end{equation}
%
the right hand side can be written as
%
\begin{equation}
- \int p_x(\mathbf{A} \mathbf{x}) \ln p_x(\mathbf{A} \mathbf{x}) \abs{\det \mathbf{A}} d\mathbf{x} - \ln \abs{\det \mathbf{A} ^ {-1}} \int p_y(\mathbf{y}) d\mathbf{y}.
\end{equation}
%
By the transformation 
%
\begin{equation}
\mathbf{x}' = \mathbf{A} \mathbf{x},
\end{equation}
%
the first term can be written as
%
\begin{equation}
- \int p_x(\mathbf{x}') \ln p_x(\mathbf{x}') d\mathbf{x}' = {\rm H}(\mathbf{x}),
\end{equation}
%
and the second term can be written as
%
\begin{equation}
- \ln \abs{\det \mathbf{A} ^ {-1}} = \ln \abs{\det \mathbf{A}}.
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm H}(\mathbf{y}) = {\rm H}(\mathbf{x}) + \ln \abs{\det \mathbf{A}}.
\end{equation}
%


\subsection{}
\label{1.33}
Let $x$ and $y$ be two discrete random variables.
By the definition,
%
\begin{equation}
{\rm H} (y | x) = - \sum_i \sum_j p(x_i, y_j) \ln p(y_j | x_i).
\end{equation}
%
If ${\rm H}(y | x)$ is zero, then
%
\begin{equation}
0 = - \sum_i p(x_i)\sum_j p(y_j | x_i) \ln p(y_j | x_i).
\end{equation}
%
Since
%
\begin{equation}
\begin{aligned}
p(x_i) &\geq 0, \\
p(y_j | x_i) \ln p(y_j | x_i) &\leq 0.
\end{aligned}
\end{equation}
%
for all $i$ and $j$, the equation reduces to
%
\begin{equation}
p(y_j | x_i) \ln p(y_j | x_i) = 0.
\end{equation}
%
Therefore, $p(y_j | x_i)$ is zero or one.
Thus, since
%
\begin{equation}
\sum_j p(y_j | x_i) = 1,
\end{equation}
%
it can be written that
%
\begin{equation}
p(y_j | x_i) = \delta_{j j'(i)},
\end{equation}
%
where $j'(i)$ is unique for each $i$.


\subsection{}
\label{1.34}
Let
%
\begin{equation}
\begin{aligned}
L(p(x)) = &- \int_{- \infty}^{\infty} p(x) \ln p(x) dx + \lambda_1 \left( \int_{- \infty}^{\infty} p(x) dx - 1 \right) \\
&+ \lambda_2 \left( \int_{- \infty}^{\infty} x p(x) dx - \mu \right) + \lambda_3 \left( \int_{-\infty}^{\infty} (x - \mu) ^ 2 p(x) dx - \sigma ^ 2 \right).
\end{aligned}
\end{equation}
%
Then
%
\begin{equation}
\frac{\delta L(p(x))}{\delta p(x)} = \int_{- \infty}^{\infty} \left( - \ln p(x) - 1 + \lambda_1 + \lambda_2 x + \lambda_3 (x - \mu) ^ 2 \right) dx.
\end{equation}
%
Setting the left hand side to zero gives
%
\begin{equation}
p(x) = \exp \left( - 1 + \lambda_1 + \lambda_2 x + \lambda_3 (x - \mu) ^ 2 \right).
\end{equation}
%
Therefore,
%
\begin{equation}
p(x) = \exp \left( - 1 + \lambda_1 - \frac{\lambda_2 ^ 2}{4 \lambda_3} + \lambda_3 \left( x - \left( \mu - \frac{\lambda_2}{2 \lambda_3} \right) \right) ^ 2 \right).
\end{equation}
%
Substituting it to 
%
\begin{equation}
\begin{aligned}
\int_{- \infty}^{\infty} p(x) dx &= 1, \\
\int_{- \infty}^{\infty} x p(x) dx &= \mu, \\
\int_{- \infty}^{\infty} (x - \mu) ^ 2 p(x) dx &= \sigma ^ 2,
\end{aligned}
\end{equation}
%
gives
%
\begin{equation}
\begin{aligned}
\exp \left( - 1 + \lambda_1 - \frac{\lambda_2 ^ 2}{4 \lambda_3} \right) \int_{- \infty}^{\infty} \exp \left( \lambda_3 \left( x - \left( \mu - \frac{\lambda_2}{2 \lambda_3} \right) \right) ^ 2 \right) dx &= 1, \\
\exp \left( - 1 + \lambda_1 - \frac{\lambda_2 ^ 2}{4 \lambda_3} \right) \int_{- \infty}^{\infty} x \exp \left( \lambda_3 \left( x - \left( \mu - \frac{\lambda_2}{2 \lambda_3} \right) \right) ^ 2 \right) dx &= \mu, \\
\exp \left( - 1 + \lambda_1 - \frac{\lambda_2 ^ 2}{4 \lambda_3} \right) \int_{- \infty}^{\infty} (x - \mu) ^ 2 \exp \left( \lambda_3 \left( x - \left( \mu - \frac{\lambda_2}{2 \lambda_3} \right) \right) ^ 2 \right) dx &= \sigma ^ 2.
\end{aligned}
\end{equation}
%
By the transformation
%
\begin{equation}
y = \sqrt{- \lambda_3} \left( x - \left( \mu - \frac{\lambda_2}{2 \lambda_3} \right) \right),
\end{equation}
%
they can be written as
%
\begin{equation}
\begin{aligned}
\exp \left( - 1 + \lambda_1 - \frac{\lambda_2 ^ 2}{4 \lambda_3} \right) \int_{- \infty}^{\infty} \exp \left( - y ^ 2 \right) (- \lambda_3) ^ {- \frac{1}{2}} dy &= 1,\\
\exp \left( - 1 + \lambda_1 - \frac{\lambda_2 ^ 2}{4 \lambda_3} \right) \int_{- \infty}^{\infty} \left( (- \lambda_3) ^ {- \frac{1}{2}} y + \mu - \frac{\lambda_2}{2 \lambda_3} \right) \exp \left( - y ^ 2 \right) (- \lambda_3) ^ {- \frac{1}{2}} dy &= \mu,\\
\exp \left( - 1 + \lambda_1 - \frac{\lambda_2 ^ 2}{4 \lambda_3} \right) \int_{- \infty}^{\infty} \left( (- \lambda_3) ^ {- \frac{1}{2}} y - \frac{\lambda_2}{2 \lambda_3} \right) ^ 2 \exp \left( - y ^ 2 \right) (- \lambda_3) ^ {- \frac{1}{2}} dy &= \sigma ^ 2.\\
\end{aligned}
\end{equation}
%
Since
%
\begin{equation}
\begin{aligned}
\int_{- \infty}^{\infty} \exp \left( - y ^ 2 \right) dy &= \Gamma \left( \frac{1}{2} \right), \\
\int_{- \infty}^{\infty} y \exp \left( - y ^ 2 \right) dy &= 0, \\
\int_{- \infty}^{\infty} y ^ 2 \exp \left( - y ^ 2 \right) dy &= \Gamma \left( \frac{3}{2} \right),
\end{aligned}
\end{equation}
%
they can be written as
%
\begin{equation}
\begin{aligned}
\exp \left( - 1 + \lambda_1 - \frac{\lambda_2 ^ 2}{4 \lambda_3} \right) (- \lambda_3) ^ {- \frac{1}{2}} \Gamma \left( \frac{1}{2} \right) &= 1, \\
\exp \left( - 1 + \lambda_1 - \frac{\lambda_2 ^ 2}{4 \lambda_3} \right) \left( \mu - \frac{\lambda_2}{2 \lambda_3} \right) (- \lambda_3) ^ {- \frac{1}{2}} \Gamma \left( \frac{1}{2} \right) &= \mu, \\
\exp \left( - 1 + \lambda_1 - \frac{\lambda_2 ^ 2}{4 \lambda_3} \right) \left( (- \lambda_3) ^ {- \frac{3}{2}} \Gamma \left( \frac{3}{2} \right) + (- \lambda_3) ^ {- \frac{1}{2}} \frac{\lambda_2 ^ 2}{4 \lambda_3 ^ 2} \Gamma \left( \frac{1}{2} \right) \right) &= \sigma ^ 2.
\end{aligned}
\end{equation}
%
Thus,
%
\begin{equation}
\begin{aligned}
\lambda_1 &= 1 - \frac{1}{2} \ln \left( 2 \pi \sigma ^ 2 \right), \\
\lambda_2 &= 0, \\
\lambda_3 &= - \frac{1}{2 \sigma ^ 2},
\end{aligned}
\end{equation}
%
so that
%
\begin{equation}
p(x) = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \exp \left( - \frac{1}{2 \sigma ^ 2} (x - \mu) ^ 2 \right).
\end{equation}
%


\subsection{}
\label{1.35}
Let $x$ be a variable under the Gaussian distribution with mean $\mu$ and variance $\sigma ^ 2$.
Then, by the definition,
%
\begin{equation}
{\rm H}(x) = - \int_{- \infty}^{\infty} \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) \ln \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx,
\end{equation}
%
where
%
\begin{equation}
\mathcal{N} \left( x | \mu, \sigma ^ 2 \right) = \left( 2 \pi \sigma ^ 2 \right) ^ {- \frac{1}{2}} \exp \left( - \frac{1}{2 \sigma ^2} (x - \mu) ^ 2 \right).
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm H}(x) = - \int_{- \infty}^{\infty} \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) \left( - \frac{1}{2} \ln \left( 2 \pi \sigma ^ 2 \right) - \frac{1}{2 \sigma ^2} (x - \mu) ^ 2 \right) dx.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\frac{1}{2} \ln \left( 2 \pi \sigma ^ 2 \right) \int_{- \infty}^{\infty} \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx + \frac{1}{2 \sigma ^2} \int_{- \infty}^{\infty} (x - \mu) ^ 2 \mathcal{N} \left( x | \mu, \sigma ^ 2 \right) dx.
\end{equation}
%
Thus,
%
\begin{equation}
{\rm H}(x) = \frac{1}{2} \left( 1 + \ln \left( 2 \pi \sigma ^ 2 \right) \right).
\end{equation}
%


\subsection{(Incomplete)}
\label{1.36}
Let $f$ be a strictly convex function.
Then, by the definition,
%
\begin{equation}
f \left( \lambda a + (1 - \lambda) b \right) \leq \lambda f(a) + (1 - \lambda) f(b),
\end{equation}
%
where $a \leq b$ and $0 \leq \lambda \leq 1$.
Let
%
\begin{equation}
x = \lambda a + (1 - \lambda) b.
\end{equation}
%
Then, the inequality can be written as
%
\begin{equation}
f(x) \leq \frac{b - x}{b - a} f(a) + \frac{x - a}{b - a} f(b).
\end{equation}
%
Let 
%
\begin{equation}
g(x) = \frac{b - x}{b - a} f(a) + \frac{x - a}{b - a} f(b) - f(x).
\end{equation}
%
Then,
%
\begin{equation}
g(x) \geq 0.
\end{equation}
%
Additionally, for $x > a$,
%
\begin{equation}
g(x) = (x - a) \left( \frac{f(b) - f(a)}{b - a} - \frac{f(x) - f(a)}{x - a} \right).
\end{equation}
%
By the mean value theorem, there exists $c$ and $y$ such that $a \leq c \leq b$, $a \leq y \leq x$ and
%
\begin{equation}
\begin{aligned}
f'(c) &= \frac{f(b) - f(a)}{b - a}, \\
f'(y) &= \frac{f(x) - f(a)}{x - a}.
\end{aligned}
\end{equation}
%
Then, for $x > a$, the inequality reduces to
%
\begin{equation}
f'(y) \leq f'(c). 
\end{equation}
%



\subsection{}
\label{1.37}
Let $\mathbf{x}$ and $\mathbf{y}$ be two variables.
Then, by the definition, 
%
\begin{equation}
{\rm H}(\mathbf{x}, \mathbf{y}) = - \int \int p(\mathbf{x}, \mathbf{y}) \ln p(\mathbf{x}, \mathbf{y}) d\mathbf{x} d\mathbf{y}.
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
\begin{aligned}
&- \int \int p(\mathbf{x}, \mathbf{y}) \left( \ln p(\mathbf{y} | \mathbf{x}) + \ln p(\mathbf{x}) \right) d\mathbf{x} d\mathbf{y} \\
&= - \int \int p(\mathbf{x}, \mathbf{y}) \ln p(\mathbf{y} | \mathbf{x}) d\mathbf{x} d\mathbf{y} - \int \left( \int p(\mathbf{x}, \mathbf{y}) d\mathbf{y} \right) \ln p(\mathbf{x}) d\mathbf{x}.
\end{aligned}
\end{equation}
%
The right hand side can be written as
%
\begin{equation}
{\rm H}(\mathbf{y} | \mathbf{x}) + {\rm H}(\mathbf{x}).
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm H}(\mathbf{x}, \mathbf{y}) = {\rm H}(\mathbf{y} | \mathbf{x}) + {\rm H}(\mathbf{x}).
\end{equation}
%


\subsection{}
\label{1.38}
Let $f$ be a strictly convex function.
Then, by the definition,
%
\begin{equation}
f \left( \lambda x_1 + (1 - \lambda) x_2 \right) \leq \lambda f(x_1) + (1 - \lambda) f(x_2),
\end{equation}
%
where $0 \leq \lambda \leq 1$.
Let us assume that 
%
\begin{equation}
f \left( \sum_{i = 1}^{M} \lambda_i x_i \right) \leq \sum_{i = 1}^{M} \lambda_i f(x_i),
\end{equation}
%
where $\lambda_i \geq 0$ and
%
\begin{equation}
\sum_{i = 1}^{M} \lambda_i = 1.
\end{equation}
%
Here, let $\lambda_i \geq 0$ and
%
\begin{equation}
\sum_{i = 1}^{M + 1} \lambda_i = 1.
\end{equation}
%
Then, by the definition,
%
\begin{equation}
f \left( \sum_{i = 1}^{M + 1} \lambda_i x_i \right) \leq \lambda_{M + 1} f ( x_{M + 1} ) + (1 - \lambda_{M + 1}) f \left( \sum_{i = 1}^{M} \frac{\lambda_i}{1 - \lambda_{M + 1}} x_i \right).
\end{equation}
%
By the assumption,
%
\begin{equation}
f \left( \sum_{i = 1}^{M} \frac{\lambda_i}{1 - \lambda_{M + 1}} x_i \right) \leq \sum_{i = 1}^{M} \frac{\lambda_i}{1 - \lambda_{M + 1}} f(x_i).
\end{equation}
%
Therefore,
%
\begin{equation}
f \left( \sum_{i = 1}^{M + 1} \lambda_i x_i \right) \leq \lambda_{M + 1} f ( x_{M + 1} ) + (1 - \lambda_{M + 1}) \sum_{i = 1}^{M} \frac{\lambda_i}{1 - \lambda_{M + 1}} f(x_i).
\end{equation}
%
Thus,
%
\begin{equation}
f \left( \sum_{i = 1}^{M + 1} \lambda_i x_i \right) \leq \sum_{i = 1}^{M + 1} \lambda_i f(x_i).
\end{equation}
%
Hence, the assumption is proved by induction on $M$.


\subsection{}
\label{1.39}
Let $x$ and $y$ be two binary variables where
%
\begin{equation}
\begin{aligned}
p(x = 0, y = 0) &= \frac{1}{3}, \\
p(x = 0, y = 1) &= \frac{1}{3}, \\
p(x = 1, y = 0) &= 0, \\
p(x = 1, y = 1) &= \frac{1}{3}.
\end{aligned}
\end{equation}
%


\subsubsection{}
By the definition,
%
\begin{equation}
{\rm H}(x) = - \sum p(x) \ln p(x).
\end{equation}
%
By the distribution,
%
\begin{equation}
\begin{aligned}
p(x = 0) &= \frac{2}{3}, \\
p(x = 1) &= \frac{1}{3}.
\end{aligned}
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm H}(x) = \ln 3 - \frac{2}{3} \ln 2.
\end{equation}
%


\subsubsection{}
By the definition,
%
\begin{equation}
{\rm H}(y) = - \sum p(y) \ln p(y).
\end{equation}
%
By the distribution,
%
\begin{equation}
\begin{aligned}
p(y = 0) &= \frac{1}{3}, \\
p(y = 1) &= \frac{2}{3}.
\end{aligned}
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm H}(y) = \ln 3 - \frac{2}{3} \ln 2.
\end{equation}
%


\subsubsection{}
By the definition,
%
\begin{equation}
{\rm H}(y | x) = - \sum p(x, y) \ln p(y | x).
\end{equation}
%
By the definition,
%
\begin{equation}
\begin{aligned}
p(y = 0 | x = 0) &= \frac{p(x = 0, y = 0)}{p(x = 0)}, \\
p(y = 0 | x = 1) &= \frac{p(x = 1, y = 0)}{p(x = 1)}, \\
p(y = 1 | x = 0) &= \frac{p(x = 0, y = 1)}{p(x = 0)}, \\
p(y = 1 | x = 1) &= \frac{p(x = 1, y = 1)}{p(x = 1)}.
\end{aligned}
\end{equation}
%
Then, by the distribution,
%
\begin{equation}
\begin{aligned}
p(y = 0 | x = 0) &= \frac{1}{2}, \\
p(y = 0 | x = 1) &= 0, \\
p(y = 1 | x = 0) &= \frac{1}{2}, \\
p(y = 1 | x = 1) &= 1.
\end{aligned}
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm H}(y | x) = \frac{2}{3} \ln 2.
\end{equation}
%


\subsubsection{}
By the definition,
%
\begin{equation}
{\rm H}(x | y) = - \sum p(x, y) \ln p(x | y).
\end{equation}
%
By the definition,
%
\begin{equation}
\begin{aligned}
p(x = 0 | y = 0) &= \frac{p(x = 0, y = 0)}{p(y = 0)}, \\
p(x = 0 | y = 1) &= \frac{p(x = 0, y = 1)}{p(y = 1)}, \\
p(x = 1 | y = 0) &= \frac{p(x = 1, y = 0)}{p(y = 0)}, \\
p(x = 1 | y = 1) &= \frac{p(x = 1, y = 1)}{p(y = 1)}.
\end{aligned}
\end{equation}
%
Then, by the distribution,
%
\begin{equation}
\begin{aligned}
p(x = 0 | y = 0) &= 1, \\
p(x = 0 | y = 1) &= \frac{1}{2}, \\
p(x = 1 | y = 0) &= 0, \\
p(x = 1 | y = 1) &= \frac{1}{2}.
\end{aligned}
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm H}(x | y) = \frac{2}{3} \ln 2.
\end{equation}
%


\subsubsection{}
By the definition,
%
\begin{equation}
{\rm H}(x, y) = - \sum p(x, y) \ln p(x, y).
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm H}(x, y) = \ln 3.
\end{equation}
%


\subsubsection{}
By the definition,
%
\begin{equation}
{\rm I}(x, y) = - \sum p(x, y) \ln \frac{p(x) p(y)}{p(x, y)}.
\end{equation}
%
By the distribution, the right hand side can be written as
%
\begin{equation}
{\rm H}(x) + {\rm H}(y) - {\rm H}(x, y).
\end{equation}
%
Therefore,
%
\begin{equation}
{\rm I}(x, y) = \ln 3- \frac{4}{3} \ln 2.
\end{equation}
%


\subsection{}
\label{1.40}
Let $\{ x_i \}$ be a set of points where $x_i > 0$, and let $\{ \lambda_i \}$ be a set of coefficients where $\lambda_i \geq 0$ and 
%
\begin{equation}
\sum_{i = 1}^{M} \lambda_i = 1.
\end{equation}
%
By Jensen's inequality,
%
\begin{equation}
\sum_{i = 1}^{M} \lambda_i \ln x_i \leq \ln \left( \sum_{i = 1}^{M} \lambda_i x_i \right).
\end{equation}
%
Therefore,
%
\begin{equation}
\prod_{i = 1}^{M} x_i ^ {\lambda_i} \leq \sum_{i = 1}^{M} \lambda_i x_i.
\end{equation}
%
Substituting
%
\begin{equation}
\lambda_i = \frac{1}{M}
\end{equation}
%
gives
%
\begin{equation}
\left( \prod_{i = 1}^{M} x_i \right) ^ \frac{1}{M} \leq \frac{1}{M} \sum_{i = 1}^{M} x_i.
\end{equation}
%
















